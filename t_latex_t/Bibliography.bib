@book{book:2008,
   title =     {Graph theory},
   author =    {Adrian Bondy, U.S.R. Murty},
   publisher = {Springer},
   isbn =      {1846289696; 9781846289699},
   year =      {2008},
   series =    {Graduate texts in mathematics 244},
   edition =   {3rd Corrected Printing.},
   }

@article{article:bollobas,
author = {Bollobas, Bela and Szemerédi, Endre},
year = {2002},
month = {01},
pages = {194 - 200},
title = {Girth of Sparse Graphs},
volume = {39},
journal = {Journal of Graph Theory},
doi = {10.1002/jgt.10023}
}

@book{book:Gary,
   title =     {A First Course in Graph Theory},
   author =    {Gary Chartrand, Ping Zhang},
   publisher = {Dover Publications},
   isbn =      {0486483681; 9780486483689},
   year =      {2012},
   series =    {Dover Books on Mathematics},
}

@book{book:Newman,
   title =     {Networks},
   author =    {Mark Newman},
   publisher = {Oxford University Press},
   isbn =      {0198805098; 9780198805090},
   year =      {2018},
   edition =   {2},
}

@book{book:algebraic,
  title={Algebraic Coding Theory and Information Theory: DIMACS Workshop, Algebraic Coding Theory and Information Theory, December 15-18, 2003, Rutgers University, Piscataway, New Jersey},
  author={Ashikhmin, A. and Barg, A.},
  isbn={9780821871102},
  series={DIMACS series in discrete mathematics and theoretical computer science},
  url={https://books.google.gr/books?id=wp7XsCAm\_9EC},
  publisher={American Mathematical Soc.}
}
@article{article:TGNNM,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  doi={10.1109/TNN.2008.2005605}}


@Inbook{book:Jost2007,
author="Jost, J{\"u}rgen",
title="Dynamical Networks",
bookTitle="Networks: From Biology to Theory",
year="2007",
publisher="Springer London",
address="London",
pages="35--62",
abstract="The theory of dynamical networks is concerned with systems of dynamical units coupled according to an underlying graph structure. It therefore investigates the interplay between dynamics and structure, between the temporal processes going on at the individual units and the static spatial structure linking them. In order to analyse that spatial structure, formalized as a graph, we discuss an essentially complete system of graph invariants, the spectrum of the graph Laplacian, and how it relates to various qualitative properties of the graph. We also describe various stochastic construction schemes for graphs with certain qualitative features. We then turn to dynamical aspects and discuss systems of oscillators with diffusive coupling according to the graph Laplacian and analyse their synchronizability. The analytical tool here are local expansions in terms of eigenmodes of the graph Laplacian. This is viewed as a first step towards a general understanding of pattern formation in systems of coupled oscillators.",
isbn="978-1-84628-780-0",
doi="10.1007/978-1-84628-780-0_3",
url="https://doi.org/10.1007/978-1-84628-780-0_3"
}


@article{article:ChebNet,
author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
year = {2016},
month = {06},
title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}
}
@article{article:kermack,
  author = {Kermack, W. O. and McKendrick, A. G.},
  month = {08},
  pages = {700-721},
  title = {A Contribution to the Mathematical Theory of Epidemics},
  doi = {10.1098/rspa.1927.0118},
  volume = {115},
  year = {1927},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences}
}
@book{book:Goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{article:McCulloch1943,
	volume = {5},
	number = {4},
	doi = {10.1007/bf02478259},
	title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
	author = {Warren S. McCulloch and Walter Pitts},
	journal = {The Bulletin of Mathematical Biophysics},
	pages = {115--133},
	year = {1943}
}

@book{book:Gurney1997AnIT,
  title={An introduction to neural networks},
  author={Kevin N. Gurney},
  year={1997}
}

@article{article:Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Frank Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        }
}
@book{book:minsky1969perceptrons,
  title={Perceptrons; an Introduction to Computational Geometry},
  author={Minsky, M. and Papert, S.},
  isbn={9780262630221},
  lccn={69014379},
  url={https://books.google.gr/books?id=Ow1OAQAAIAAJ},
  year={1969},
  publisher={MIT Press}
}

@book{book:werbos1975beyond,
  title={Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  author={Werbos, P.J.},
  url={https://books.google.gr/books?id=z81XmgEACAAJ},
  year={1975},
  publisher={Harvard University}
}

@INPROCEEDINGS{article:Cresceptron,

  author={Weng, J. and Ahuja, N. and Huang, T.S.},

  booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks}, 

  title={Cresceptron: a self-organizing neural network which grows adaptively}, 

  year={1992},

  volume={1},

  number={},

  pages={576-581 vol.1},

  doi={10.1109/IJCNN.1992.287150}}

@article{article:SCHMID,
title = {Deep learning in neural networks: An overview},
journal = {Neural Networks},
volume = {61},
pages = {85-117},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
author = {Jürgen Schmidhuber},
keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@article{article:Cauchy,
        title = {Méthode générale pour la résolution des systèmes d'équations simultanées},
        author = {A. Cauchy},
        year = {1847},
        journal = {C. R. Acad. Sci},
        volume = {25},
        pages = {536-538}}


@book{book:optimization,
   title =     {Optimization and control with applications},
   author =    {Liqun Qi, Kok Lay Teo, Xiao Qi Yang},
   publisher = {Springer},
   isbn =      {9780387242545; 0387242546},
   year =      {2005},
   series =    {Applied Optimization},
   edition =   {1}}

@book{book:nielsen,
    title = {Neural Networks and Deep Learning},
    author = {Michael A. Nielsen},
    year = {2015},
    publisher = {Determination Press},
    url = {http://neuralnetworksanddeeplearning.com},
}
@book{book:Abramowitz,
    title = {Abramowitz and Stegun},
    author = {Milton Abramowitz and Irene Stegun},
    year = {1972},
    publisher = {Bracewell 200},
}

@InProceedings{article:bottou,
author="Bottou, L{\'e}on",
editor="Lechevallier, Yves
and Saporta, Gilbert",
title="Large-Scale Machine Learning with Stochastic Gradient Descent",
booktitle="Proceedings of COMPSTAT'2010",
year="2010",
publisher="Physica-Verlag HD",
address="Heidelberg",
pages="177--186",
abstract="During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
isbn="978-3-7908-2604-3"
}

@incollection{book:robbins,
title = {A CONVERGENCE THEOREM FOR NON NEGATIVE ALMOST SUPERMARTINGALES AND SOME APPLICATIONS**Research supported by NIH Grant 5-R01-GM-16895-03 and ONR Grant N00014-67-A-0108-0018.},
editor = {Jagdish S. Rustagi},
booktitle = {Optimizing Methods in Statistics},
publisher = {Academic Press},
pages = {233-257},
year = {1971},
isbn = {978-0-12-604550-5},
doi = {https://doi.org/10.1016/B978-0-12-604550-5.50015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780126045505500158},
author = {H. Robbins and D. Siegmund},
abstract = {Publisher Summary
This chapter discusses a convergence theorem for nonnegative almost supermartingales and some applications. It discusses a unified treatment of a number of almost sure convergence theorems by exploiting the fact that the processes involved possess a common almost supermartingale properties. The inequalities are simple and useful generalizations of well-known results in martingale theory. Dvoretzky proved a general convergence theorem that includes Blum's result for the Robbins–Monro process and the corresponding result for the Kiefer–Wolfowitz method for estimating the maximum of a regression function as special cases.}
}

@article{article:image_gradient,
author = {Carpenter, Kristy and Cohen, David and Jarrell, Juliet and Huang, Xudong},
year = {2018},
month = {10},
pages = {},
title = {Deep learning and virtual drug screening},
volume = {10},
journal = {Future Medicinal Chemistry},
doi = {10.4155/fmc-2018-0314}
}

@article{article:backprop,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}

@book{book:horn,
place={Cambridge}, title={Matrix Analysis},
DOI={10.1017/CBO9780511810817},
publisher={Cambridge University Press},
author={Horn, Roger A. and Johnson, Charles R.},
year={1985}
}

@book{book:WerbosBackprop,
author = {Werbos, Paul John},
title = {The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting},
year = {1994},
isbn = {0471598976},
publisher = {Wiley-Interscience},
address = {USA}
}

@ARTICLE{article:lecun1998,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}

@article{article:xiong,
title = {Graph neural networks for automated de novo drug design},
journal = {Drug Discovery Today},
volume = {26},
number = {6},
pages = {1382-1393},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621000787},
author = {Jiacheng Xiong and Zhaoping Xiong and Kaixian Chen and Hualiang Jiang and Mingyue Zheng},
abstract = {The goal of de novo drug design is to create novel chemical entities with desired biological activities and pharmacokinetics (PK) properties. Over recent years, with the development of artificial intelligence (AI) technologies, data-driven methods have rapidly gained in popularity in this field. Among them, graph neural networks (GNNs), a type of neural network directly operating on the graph structure data, have received extensive attention. In this review, we introduce the applications of GNNs in de novo drug design from three aspects: molecule scoring, molecule generation and optimization, and synthesis planning. Furthermore, we also discuss the current challenges and future directions of GNNs in de novo drug design.}
}

@article{article:bognini,
title = {Molecular generative Graph Neural Networks for Drug Discovery},
journal = {Neurocomputing},
volume = {450},
pages = {242-252},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005737},
author = {Pietro Bongini and Monica Bianchini and Franco Scarselli},
keywords = {Graph generation, Molecule generation, Deep learning, Graph Neural Networks, Drug Discovery},
abstract = {Drug Discovery is a fundamental and ever-evolving field of research. The design of new candidate molecules requires large amounts of time and money, and computational methods are being increasingly employed to cut these costs. Machine learning methods are ideal for the design of large amounts of potential new candidate molecules, which are naturally represented as graphs. Graph generation is being revolutionized by deep learning methods, and molecular generation is one of its most promising applications. In this paper, we introduce a sequential molecular graph generator based on a set of graph neural network modules, which we call MG2N2. At each step, a node or a group of nodes is added to the graph, along with its connections. The modular architecture simplifies the training procedure, also allowing an independent retraining of a single module. Sequentiality and modularity make the generation process interpretable. The use of Graph Neural Networks maximizes the information in input at each generative step, which consists of the subgraph produced during the previous steps. Experiments of unconditional generation on the QM9 and Zinc datasets show that our model is capable of generalizing molecular patterns seen during the training phase, without overfitting. The results indicate that our method is competitive, and outperforms challenging baselines for unconditional generation.}
}

@article{article:wieder,
title = {A compact review of molecular property prediction with graph neural networks},
journal = {Drug Discovery Today: Technologies},
volume = {37},
pages = {1-12},
year = {2020},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1740674920300305},
author = {Oliver Wieder and Stefan Kohlbacher and Mélaine Kuenemann and Arthur Garon and Pierre Ducrot and Thomas Seidel and Thierry Langer},
keywords = {AI, Deep-learning, Neural-networks, Graph neural-networks, Molecular representation, Molecular property, Drug discovery, Computational chemistry},
abstract = {As graph neural networks are becoming more and more powerful and useful in the field of drug discovery, many pharmaceutical companies are getting interested in utilizing these methods for their own in-house frameworks. This is especially compelling for tasks such as the prediction of molecular properties which is often one of the most crucial tasks in computer-aided drug discovery workflows. The immense hype surrounding these kinds of algorithms has led to the development of many different types of promising architectures and in this review we try to structure this highly dynamic field of AI-research by collecting and classifying 80 GNNs that have been used to predict more than 20 molecular properties using 48 different datasets.}
}


@InProceedings{article:sanchez,
  title = 	 {Learning to Simulate Complex Physics with Graph Networks},
  author =       {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8459--8468},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sanchez-gonzalez20a/sanchez-gonzalez20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},
  abstract = 	 {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework—which we term "Graph Network-based Simulators" (GNS)—represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.}
}
@ARTICLE{article:weng,
       author = {{Weng Lo}, Wai and {Layeghy}, Siamak and {Sarhan}, Mohanad and {Gallagher}, Marcus and {Portmann}, Marius},
        title = "{E-GraphSAGE: A Graph Neural Network based Intrusion Detection System for IoT}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = 2021,
        month = mar,
          eid = {arXiv:2103.16329},
        pages = {arXiv:2103.16329},
archivePrefix = {arXiv},
       eprint = {2103.16329},
 primaryClass = {cs.NI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210316329W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{article:zhiwei,
title = {Deep Graph neural network-based spammer detection under the perspective of heterogeneous cyberspace},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {205-218},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330612},
author = {Zhiwei Guo and Lianggui Tang and Tan Guo and Keping Yu and Mamoun Alazab and Andrii Shalaginov},
keywords = {Cyberspace security, Spammer detection, Graph neural network, Heterogeneous social graph},
abstract = {Due to the severe threat to cyberspace security, detection of online spammers has been a universal concern of academia. Nowadays, prevailing literature of this field almost leveraged various relations to enhance feature spaces. However, they majorly focused stable or visible relations, yet neglected the existence of those which are generated occasionally. Exactly, some latent feature components can be extracted from the view of heterogeneous information networks. Thus, this paper proposes a Deep Graph neural network-based Spammer detection (DeG-Spam) model under the perspective of heterogeneous cyberspace. Specifically, representations for occasional relations and inherent relations are separately modelled. Based on this, a graph neural network framework is formulated to generate feature expressions for the social graph. With more feature components being mined, acquirement of stronger and more comprehensive feature spaces ensures the accuracy of spammer detection. At last, fruitful experiments are carried out on two benchmark datasets to compare the DeG-Spam with typical spammer detection approaches. Experimental results show that it performs about 5\%–10\% better than baselines.}
}

@INPROCEEDINGS{article:chaudhary,

  author={Chaudhary, Anshika and Mittal, Himangi and Arora, Anuja},

  booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)}, 

  title={Anomaly Detection using Graph Neural Networks}, 

  year={2019},

  volume={},

  number={},

  pages={346-350},

  doi={10.1109/COMITCon.2019.8862186}}

@article{article:monti,
Author = {Federico Monti and Fabrizio Frasca and Davide Eynard and Damon Mannion and Michael M. Bronstein},
Title = {Fake News Detection on Social Media using Geometric Deep Learning},
Year = {2019},
Eprint = {arXiv:1902.06673},}


@misc{article:jiang,
Author = {Weiwei Jiang and Jiayun Luo},
Title = {Graph Neural Network for Traffic Forecasting: A Survey},
Year = {2021},
Eprint = {arXiv:2101.11174},
}

@inproceedings{article:duvenaud,
 author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
 url = {https://proceedings.neurips.cc/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf},
 volume = {28},
 year = {2015}
}

@article{article:stefi,
author = {Stefi, Nouleho and Barth, Dominique and David, Olivier and Quessette, Franck and Weisser, Marc-Antoine and Watel, Dimitri},
year = {2019},
month = {12},
pages = {e0226680},
title = {Improving graphs of cycles approach to structural similarity of molecules},
volume = {14},
journal = {PLOS ONE},
doi = {10.1371/journal.pone.0226680}
}

@article{article:zhou,
title = {Graph neural networks: A review of methods and applications},
journal = {AI Open},
volume = {1},
pages = {57-81},
year = {2020},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
keywords = {Deep learning, Graph neural network},
abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.}
}
@ARTICLE{article:sperduti, author={Sperduti, A. and Starita, A.}, journal={IEEE
                  Transactions on Neural Networks}, title={Supervised
                  neural networks for the classification of
                  structures}, year={1997}, volume={8}, number={3},
                  pages={714-735}, doi={10.1109/72.572108}}


@Article{article:lecun2015,
author={LeCun, Yann
and Bengio, Yoshua
and Hinton, Geoffrey},
title={Deep learning},
journal={Nature},
year={2015},
month={May},
day={01},
volume={521},
number={7553},
pages={436-444},
abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
issn={1476-4687},
doi={10.1038/nature14539},
url={https://doi.org/10.1038/nature14539}
}

@ARTICLE{article:geomDeep, author={Bronstein, Michael M. and Bruna, Joan and
                  LeCun, Yann and Szlam, Arthur and Vandergheynst,
                  Pierre}, journal={IEEE Signal Processing Magazine},
                  title={Geometric Deep Learning: Going beyond
                  Euclidean data}, year={2017}, volume={34},
                  number={4}, pages={18-42},
                  doi={10.1109/MSP.2017.2693418}}

% embedding
@ARTICLE{article:Cui, author={Cui, Peng and Wang, Xiao and Pei, Jian and
                  Zhu, Wenwu}, journal={IEEE Transactions on Knowledge
                  and Data Engineering}, title={A Survey on Network
                  Embedding}, year={2019}, volume={31}, number={5},
                  pages={833-852}, doi={10.1109/TKDE.2018.2849727}}



@article{hamilton2017representation,
  title={Representation learning on graphs: Methods and applications},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  journal={arXiv preprint arXiv:1709.05584},
  year={2017}
}

@misc{article:zhang,
Author = {Daokun Zhang and Jie Yin and Xingquan Zhu and Chengqi Zhang},
Title = {Network Representation Learning: A Survey},
Year = {2017},
Eprint = {arXiv:1801.05852},
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{xu2018powerful,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}

@misc{site:embeddings,
title={Node Representation Learning},
url={https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/node-representation-learning},
accessed={June 26, 2022}
}

@book{book:golub,
   title =     {Matrix Computations},
   author =    {Gene H. Golub, Charles F. Van Loan},
   publisher = {Johns Hopkins University Press},
   isbn =      {1421407949; 9781421407944},
   year =      {2012},
   series =    {Johns Hopkins Studies in the Mathematical Sciences},
   edition =   {fourth},
   }

@article{defferrard2016convolutional,
  title={Convolutional neural networks on graphs with fast localized spectral filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{article:hammond,
title = {Wavelets on graphs via spectral graph theory},
journal = {Applied and Computational Harmonic Analysis},
volume = {30},
number = {2},
pages = {129-150},
year = {2011},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2010.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1063520310000552},
author = {David K. Hammond and Pierre Vandergheynst and Rémi Gribonval},
keywords = {Graph theory, Wavelets, Spectral graph theory, Overcomplete wavelet frames},
abstract = {We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian L. Given a wavelet generating kernel g and a scale parameter t, we define the scaled wavelet operator Tgt=g(tL). The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on g, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing L. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains.}
}


@InProceedings{article:messageP,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}

@inproceedings{article:attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{velickovic2017graph,
  title={Graph attention networks},
  author={Velickovic, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={stat},
  volume={1050},
  pages={20},
  year={2017}
}

@InProceedings{article:setAttention,
  title = 	 {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author =       {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3744--3753},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lee19d.html},
  abstract = 	 {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.}
}

@article{daigavane2021understanding,
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  title = {Understanding Convolutions on Graphs},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/understanding-gnns},
  doi = {10.23915/distill.00032}
}

@inproceedings{zhang2018end,
  title={An end-to-end deep learning architecture for graph classification},
  author={Zhang, Muhan and Cui, Zhicheng and Neumann, Marion and Chen, Yixin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}
@article{ying2018hierarchical,
  title={Hierarchical graph representation learning with differentiable pooling},
  author={Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{lee2019self,
  title={Self-attention graph pooling},
  author={Lee, Junhyun and Lee, Inyeop and Kang, Jaewoo},
  booktitle={International conference on machine learning},
  pages={3734--3743},
  year={2019},
  organization={PMLR}
}

@article{murphy,
  title={Deep learning of contagion dynamics on complex networks},
  author={Murphy, Charles and Laurence, Edward and Allard, Antoine},
  journal={Nature Communications},
  volume={12},
  number={1},
  pages={1--11},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{albert2002statistical,
  title={Statistical mechanics of complex networks},
  author={Albert, R{\'e}ka and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  journal={Reviews of modern physics},
  volume={74},
  number={1},
  pages={47},
  year={2002},
  publisher={APS}
}

@article{erdHos1960evolution,
  title={On the evolution of random graphs},
  author={Erd{\H{o}}s, Paul and R{\'e}nyi, Alfr{\'e}d and others},
  journal={Publ. Math. Inst. Hung. Acad. Sci},
  volume={5},
  number={1},
  pages={17--60},
  year={1960}
}

@article{article:gilbert,
author = {E. N. Gilbert},
title = {{Random Graphs}},
volume = {30},
journal = {The Annals of Mathematical Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {1141 -- 1144},
year = {1959},
doi = {10.1214/aoms/1177706098},
URL = {https://doi.org/10.1214/aoms/1177706098}
}