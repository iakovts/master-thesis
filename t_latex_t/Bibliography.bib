@book{book:2008,
   title =     {Graph theory},
   author =    {Adrian Bondy, U.S.R. Murty},
   publisher = {Springer},
   isbn =      {1846289696; 9781846289699},
   year =      {2008},
   series =    {Graduate texts in mathematics 244},
   edition =   {3rd Corrected Printing.},
   }

@article{article:bollobas,
author = {Bollobas, Bela and Szemerédi, Endre},
year = {2002},
month = {01},
pages = {194 - 200},
title = {Girth of Sparse Graphs},
volume = {39},
journal = {Journal of Graph Theory},
doi = {10.1002/jgt.10023}
}

@book{book:Gary,
   title =     {A First Course in Graph Theory},
   author =    {Gary Chartrand, Ping Zhang},
   publisher = {Dover Publications},
   isbn =      {0486483681; 9780486483689},
   year =      {2012},
   series =    {Dover Books on Mathematics},
}

@book{book:Newman,
   title =     {Networks},
   author =    {Mark Newman},
   publisher = {Oxford University Press},
   isbn =      {0198805098; 9780198805090},
   year =      {2018},
   edition =   {2},
}

@book{book:algebraic,
  title={Algebraic Coding Theory and Information Theory: DIMACS Workshop, Algebraic Coding Theory and Information Theory, December 15-18, 2003, Rutgers University, Piscataway, New Jersey},
  author={Ashikhmin, A. and Barg, A.},
  isbn={9780821871102},
  series={DIMACS series in discrete mathematics and theoretical computer science},
  url={https://books.google.gr/books?id=wp7XsCAm\_9EC},
  publisher={American Mathematical Soc.}
}
@article{article:TGNNM,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  doi={10.1109/TNN.2008.2005605}}


@Inbook{book:Jost2007,
author="Jost, J{\"u}rgen",
title="Dynamical Networks",
bookTitle="Networks: From Biology to Theory",
year="2007",
publisher="Springer London",
address="London",
pages="35--62",
abstract="The theory of dynamical networks is concerned with systems of dynamical units coupled according to an underlying graph structure. It therefore investigates the interplay between dynamics and structure, between the temporal processes going on at the individual units and the static spatial structure linking them. In order to analyse that spatial structure, formalized as a graph, we discuss an essentially complete system of graph invariants, the spectrum of the graph Laplacian, and how it relates to various qualitative properties of the graph. We also describe various stochastic construction schemes for graphs with certain qualitative features. We then turn to dynamical aspects and discuss systems of oscillators with diffusive coupling according to the graph Laplacian and analyse their synchronizability. The analytical tool here are local expansions in terms of eigenmodes of the graph Laplacian. This is viewed as a first step towards a general understanding of pattern formation in systems of coupled oscillators.",
isbn="978-1-84628-780-0",
doi="10.1007/978-1-84628-780-0_3",
url="https://doi.org/10.1007/978-1-84628-780-0_3"
}


@article{article:ChebNet,
author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
year = {2016},
month = {06},
title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}
}
@article{article:kermack,
  author = {Kermack, W. O. and McKendrick, A. G.},
  month = {08},
  pages = {700-721},
  title = {A Contribution to the Mathematical Theory of Epidemics},
  doi = {10.1098/rspa.1927.0118},
  volume = {115},
  year = {1927},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences}
}
@book{book:Goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{article:McCulloch1943,
	volume = {5},
	number = {4},
	doi = {10.1007/bf02478259},
	title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
	author = {Warren S. McCulloch and Walter Pitts},
	journal = {The Bulletin of Mathematical Biophysics},
	pages = {115--133},
	year = {1943}
}

@book{book:Gurney1997AnIT,
  title={An introduction to neural networks},
  author={Kevin N. Gurney},
  year={1997}
}

@article{article:Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Frank Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        }
}
@book{book:minsky1969perceptrons,
  title={Perceptrons; an Introduction to Computational Geometry},
  author={Minsky, M. and Papert, S.},
  isbn={9780262630221},
  lccn={69014379},
  url={https://books.google.gr/books?id=Ow1OAQAAIAAJ},
  year={1969},
  publisher={MIT Press}
}

@book{book:werbos1975beyond,
  title={Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  author={Werbos, P.J.},
  url={https://books.google.gr/books?id=z81XmgEACAAJ},
  year={1975},
  publisher={Harvard University}
}

@INPROCEEDINGS{article:Cresceptron,

  author={Weng, J. and Ahuja, N. and Huang, T.S.},

  booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks}, 

  title={Cresceptron: a self-organizing neural network which grows adaptively}, 

  year={1992},

  volume={1},

  number={},

  pages={576-581 vol.1},

  doi={10.1109/IJCNN.1992.287150}}

@article{article:SCHMID,
title = {Deep learning in neural networks: An overview},
journal = {Neural Networks},
volume = {61},
pages = {85-117},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
author = {Jürgen Schmidhuber},
keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@article{article:Cauchy,
        title = {Méthode générale pour la résolution des systèmes d'équations simultanées},
        author = {A. Cauchy},
        year = {1847},
        journal = {C. R. Acad. Sci},
        volume = {25},
        pages = {536-538}}


@book{book:optimization,
   title =     {Optimization and control with applications},
   author =    {Liqun Qi, Kok Lay Teo, Xiao Qi Yang},
   publisher = {Springer},
   isbn =      {9780387242545; 0387242546},
   year =      {2005},
   series =    {Applied Optimization},
   edition =   {1}}

@book{book:nielsen,
    title = {Neural Networks and Deep Learning},
    author = {Michael A. Nielsen},
    year = {2015},
    publisher = {Determination Press},
    url = {http://neuralnetworksanddeeplearning.com},
}
@book{book:Abramowitz,
    title = {Abramowitz and Stegun},
    author = {Milton Abramowitz and Irene Stegun},
    year = {1972},
    publisher = {Bracewell 200},
}

@InProceedings{article:bottou,
author="Bottou, L{\'e}on",
editor="Lechevallier, Yves
and Saporta, Gilbert",
title="Large-Scale Machine Learning with Stochastic Gradient Descent",
booktitle="Proceedings of COMPSTAT'2010",
year="2010",
publisher="Physica-Verlag HD",
address="Heidelberg",
pages="177--186",
abstract="During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
isbn="978-3-7908-2604-3"
}

@incollection{book:robbins,
title = {A CONVERGENCE THEOREM FOR NON NEGATIVE ALMOST SUPERMARTINGALES AND SOME APPLICATIONS**Research supported by NIH Grant 5-R01-GM-16895-03 and ONR Grant N00014-67-A-0108-0018.},
editor = {Jagdish S. Rustagi},
booktitle = {Optimizing Methods in Statistics},
publisher = {Academic Press},
pages = {233-257},
year = {1971},
isbn = {978-0-12-604550-5},
doi = {https://doi.org/10.1016/B978-0-12-604550-5.50015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780126045505500158},
author = {H. Robbins and D. Siegmund},
abstract = {Publisher Summary
This chapter discusses a convergence theorem for nonnegative almost supermartingales and some applications. It discusses a unified treatment of a number of almost sure convergence theorems by exploiting the fact that the processes involved possess a common almost supermartingale properties. The inequalities are simple and useful generalizations of well-known results in martingale theory. Dvoretzky proved a general convergence theorem that includes Blum's result for the Robbins–Monro process and the corresponding result for the Kiefer–Wolfowitz method for estimating the maximum of a regression function as special cases.}
}

@article{article:image_gradient,
author = {Carpenter, Kristy and Cohen, David and Jarrell, Juliet and Huang, Xudong},
year = {2018},
month = {10},
pages = {},
title = {Deep learning and virtual drug screening},
volume = {10},
journal = {Future Medicinal Chemistry},
doi = {10.4155/fmc-2018-0314}
}

@article{article:backprop,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}

@book{book:horn,
place={Cambridge}, title={Matrix Analysis},
DOI={10.1017/CBO9780511810817},
publisher={Cambridge University Press},
author={Horn, Roger A. and Johnson, Charles R.},
year={1985}
}

@book{book:WerbosBackprop,
author = {Werbos, Paul John},
title = {The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting},
year = {1994},
isbn = {0471598976},
publisher = {Wiley-Interscience},
address = {USA}
}

@ARTICLE{article:lecun1998,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}

@article{article:xiong,
title = {Graph neural networks for automated de novo drug design},
journal = {Drug Discovery Today},
volume = {26},
number = {6},
pages = {1382-1393},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621000787},
author = {Jiacheng Xiong and Zhaoping Xiong and Kaixian Chen and Hualiang Jiang and Mingyue Zheng},
abstract = {The goal of de novo drug design is to create novel chemical entities with desired biological activities and pharmacokinetics (PK) properties. Over recent years, with the development of artificial intelligence (AI) technologies, data-driven methods have rapidly gained in popularity in this field. Among them, graph neural networks (GNNs), a type of neural network directly operating on the graph structure data, have received extensive attention. In this review, we introduce the applications of GNNs in de novo drug design from three aspects: molecule scoring, molecule generation and optimization, and synthesis planning. Furthermore, we also discuss the current challenges and future directions of GNNs in de novo drug design.}
}

@article{article:bognini,
title = {Molecular generative Graph Neural Networks for Drug Discovery},
journal = {Neurocomputing},
volume = {450},
pages = {242-252},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005737},
author = {Pietro Bongini and Monica Bianchini and Franco Scarselli},
keywords = {Graph generation, Molecule generation, Deep learning, Graph Neural Networks, Drug Discovery},
abstract = {Drug Discovery is a fundamental and ever-evolving field of research. The design of new candidate molecules requires large amounts of time and money, and computational methods are being increasingly employed to cut these costs. Machine learning methods are ideal for the design of large amounts of potential new candidate molecules, which are naturally represented as graphs. Graph generation is being revolutionized by deep learning methods, and molecular generation is one of its most promising applications. In this paper, we introduce a sequential molecular graph generator based on a set of graph neural network modules, which we call MG2N2. At each step, a node or a group of nodes is added to the graph, along with its connections. The modular architecture simplifies the training procedure, also allowing an independent retraining of a single module. Sequentiality and modularity make the generation process interpretable. The use of Graph Neural Networks maximizes the information in input at each generative step, which consists of the subgraph produced during the previous steps. Experiments of unconditional generation on the QM9 and Zinc datasets show that our model is capable of generalizing molecular patterns seen during the training phase, without overfitting. The results indicate that our method is competitive, and outperforms challenging baselines for unconditional generation.}
}

@article{article:wieder,
title = {A compact review of molecular property prediction with graph neural networks},
journal = {Drug Discovery Today: Technologies},
volume = {37},
pages = {1-12},
year = {2020},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1740674920300305},
author = {Oliver Wieder and Stefan Kohlbacher and Mélaine Kuenemann and Arthur Garon and Pierre Ducrot and Thomas Seidel and Thierry Langer},
keywords = {AI, Deep-learning, Neural-networks, Graph neural-networks, Molecular representation, Molecular property, Drug discovery, Computational chemistry},
abstract = {As graph neural networks are becoming more and more powerful and useful in the field of drug discovery, many pharmaceutical companies are getting interested in utilizing these methods for their own in-house frameworks. This is especially compelling for tasks such as the prediction of molecular properties which is often one of the most crucial tasks in computer-aided drug discovery workflows. The immense hype surrounding these kinds of algorithms has led to the development of many different types of promising architectures and in this review we try to structure this highly dynamic field of AI-research by collecting and classifying 80 GNNs that have been used to predict more than 20 molecular properties using 48 different datasets.}
}


@InProceedings{article:sanchez,
  title = 	 {Learning to Simulate Complex Physics with Graph Networks},
  author =       {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8459--8468},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sanchez-gonzalez20a/sanchez-gonzalez20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},
  abstract = 	 {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework—which we term "Graph Network-based Simulators" (GNS)—represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.}
}
@ARTICLE{article:weng,
       author = {{Weng Lo}, Wai and {Layeghy}, Siamak and {Sarhan}, Mohanad and {Gallagher}, Marcus and {Portmann}, Marius},
        title = "{E-GraphSAGE: A Graph Neural Network based Intrusion Detection System for IoT}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = 2021,
        month = mar,
          eid = {arXiv:2103.16329},
        pages = {arXiv:2103.16329},
archivePrefix = {arXiv},
       eprint = {2103.16329},
 primaryClass = {cs.NI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210316329W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{article:zhiwei,
title = {Deep Graph neural network-based spammer detection under the perspective of heterogeneous cyberspace},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {205-218},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330612},
author = {Zhiwei Guo and Lianggui Tang and Tan Guo and Keping Yu and Mamoun Alazab and Andrii Shalaginov},
keywords = {Cyberspace security, Spammer detection, Graph neural network, Heterogeneous social graph},
abstract = {Due to the severe threat to cyberspace security, detection of online spammers has been a universal concern of academia. Nowadays, prevailing literature of this field almost leveraged various relations to enhance feature spaces. However, they majorly focused stable or visible relations, yet neglected the existence of those which are generated occasionally. Exactly, some latent feature components can be extracted from the view of heterogeneous information networks. Thus, this paper proposes a Deep Graph neural network-based Spammer detection (DeG-Spam) model under the perspective of heterogeneous cyberspace. Specifically, representations for occasional relations and inherent relations are separately modelled. Based on this, a graph neural network framework is formulated to generate feature expressions for the social graph. With more feature components being mined, acquirement of stronger and more comprehensive feature spaces ensures the accuracy of spammer detection. At last, fruitful experiments are carried out on two benchmark datasets to compare the DeG-Spam with typical spammer detection approaches. Experimental results show that it performs about 5\%–10\% better than baselines.}
}

@INPROCEEDINGS{article:chaudhary,

  author={Chaudhary, Anshika and Mittal, Himangi and Arora, Anuja},

  booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)}, 

  title={Anomaly Detection using Graph Neural Networks}, 

  year={2019},

  volume={},

  number={},

  pages={346-350},

  doi={10.1109/COMITCon.2019.8862186}}

@article{article:monti,
Author = {Federico Monti and Fabrizio Frasca and Davide Eynard and Damon Mannion and Michael M. Bronstein},
Title = {Fake News Detection on Social Media using Geometric Deep Learning},
Year = {2019},
Eprint = {arXiv:1902.06673},}


@misc{article:jiang,
Author = {Weiwei Jiang and Jiayun Luo},
Title = {Graph Neural Network for Traffic Forecasting: A Survey},
Year = {2021},
Eprint = {arXiv:2101.11174},
}

@inproceedings{article:duvenaud,
 author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
 url = {https://proceedings.neurips.cc/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf},
 volume = {28},
 year = {2015}
}

@article{article:stefi,
author = {Stefi, Nouleho and Barth, Dominique and David, Olivier and Quessette, Franck and Weisser, Marc-Antoine and Watel, Dimitri},
year = {2019},
month = {12},
pages = {e0226680},
title = {Improving graphs of cycles approach to structural similarity of molecules},
volume = {14},
journal = {PLOS ONE},
doi = {10.1371/journal.pone.0226680}
}

@article{article:zhou,
title = {Graph neural networks: A review of methods and applications},
journal = {AI Open},
volume = {1},
pages = {57-81},
year = {2020},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
keywords = {Deep learning, Graph neural network},
abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.}
}
@ARTICLE{article:sperduti, author={Sperduti, A. and Starita, A.}, journal={IEEE
                  Transactions on Neural Networks}, title={Supervised
                  neural networks for the classification of
                  structures}, year={1997}, volume={8}, number={3},
                  pages={714-735}, doi={10.1109/72.572108}}


@Article{article:lecun2015,
author={LeCun, Yann
and Bengio, Yoshua
and Hinton, Geoffrey},
title={Deep learning},
journal={Nature},
year={2015},
month={May},
day={01},
volume={521},
number={7553},
pages={436-444},
abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
issn={1476-4687},
doi={10.1038/nature14539},
url={https://doi.org/10.1038/nature14539}
}

@ARTICLE{article:geomDeep, author={Bronstein, Michael M. and Bruna, Joan and
                  LeCun, Yann and Szlam, Arthur and Vandergheynst,
                  Pierre}, journal={IEEE Signal Processing Magazine},
                  title={Geometric Deep Learning: Going beyond
                  Euclidean data}, year={2017}, volume={34},
                  number={4}, pages={18-42},
                  doi={10.1109/MSP.2017.2693418}}
