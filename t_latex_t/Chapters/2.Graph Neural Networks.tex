\chapter{Graph Neural Networks} \label{literature}

\section{Background - Artificial Neural Networks}

\subsection{Introduction}

\subsubsection{Historical Background}

\textbf{Artificial Neural Networks (ANN)}, or sometimes simply called
\textbf{neural networks} is a class of computational models that mimic
the way biological neural networks work, such as the human brain. Interest
on the subject sparked after the seminal paper \textit{"A Logical
Calculus of the Ideas Immanent in Nervous Activity"}
\cite{article:McCulloch1943} by Warren McCulloch and Walter Pitts, where
they proposed a computationally functional model of neural networks.
Their suggestions showed that in principle, any function a digital computer
compute, a neural net should too. The models they described had weights
and thresholds, but they lacked a training method. 

The suggestions of McCulloch and Pitts lead Frank Rosenblatt to create
the \textit{Perceptron} in 1958 \cite{article:Rosenblatt1958ThePa}, a
binary classifier algorithm based on supervised
learning\footnote{Supervised learning is a machine learning training
technique that optimizes a model based on examples input-output
pairs.}. Although initially promising, single layer perceptrons were
not able to train on multiple classes of patterns and were eventually
proven incapable of learning a XOR function \footnote{XOR (Exclusive
or, $x\oplus y$) is a logical operation that is true only if its
arguments differ.} in the book \textit{Perceptrons}
\cite{book:minsky1969perceptrons}, as the way they worked was by
``separating'' data linearly. This lead to a stagnation in machine
learning research dubbed ``AI winter'', until the proposal of
\textbf{backpropagation\footnote{Backpropagation is a method of fine
tuning a neural network based on the error rate obtained from previous
runs of the program. It will be discussed in detail later in this
thesis.}} by Paul John Werbos in 1975 \cite{book:werbos1975beyond}.

A renewed interest in the field lead to the development of the
Cresceptron \cite{article:Cresceptron} in 1992, a method of training
large networks with pooling layers (\textbf{max-pooling}) and down-sampling.
GPU\footnote{GPU - Graphics Processing Units is a specialized
electronic circuit, a central part of modern computers which excels in
efficient computation of algorithms which process large blocks of data
parallelly, thus exceling in machine learning applications.} usage made
possible the training of larger networks, while new types of networks
emerged such as the \textbf{Recurrent Neural Networks (RNNs)}.
\textbf{Convolutional Neural Networks} have recently proven to be
far superior for image classification tasks.

In recent years, neural 


\subsubsection{Summary}

One can think of ANNs as a directed graph, with a collection of nodes
which are densely connected (called \textbf{artificial neurons}),
transimiting signals to each other. These nodes are usually
organized in sets of layers, with signals moving in one direction
(i.e. \textit{feed forward networks}) through weighted connections.
Signals received on a single neuron are real numbers, and the output
of a single neuron is the output of an aggregation of a non-linear function
of the sum of its inputs. Thus, each neuron can be thought as a simple
processing unit.  The weighted connections between nodes might have an
excitatory or inhibitory effect, based on these weights which can be
positive, negative or very close to zero, having no effect.
\cite[Chap. ~ 1]{book:Gurney1997AnIT}.


\newpage
\subsection{Convolutional Neural Networks}