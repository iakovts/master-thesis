\chapter{Neural Networks} \label{literature}

% \section{Background - Artificial Neural Networks}

% \subsection{Introduction}

\section{Historical Background}

\textbf{Artificial Neural Networks (ANN)}, or sometimes simply called
\textbf{neural networks} is a class of computational models that mimic
the way biological neural networks work, such as the human brain. Interest
on the subject sparked after the seminal paper \textit{"A Logical
Calculus of the Ideas Immanent in Nervous Activity"}
\cite{article:McCulloch1943} by Warren McCulloch and Walter Pitts, where
they proposed a computationally functional model of neural networks.
Their suggestions showed that in principle, any function a digital computer
can compute, a neural net should too. The models they described had weights
and thresholds, but they lacked a training method. 

The suggestions of McCulloch and Pitts lead Frank Rosenblatt to create
the \textit{Perceptron} in 1958 \cite{article:Rosenblatt1958ThePA}, a
binary classifier algorithm based on supervised
learning\footnote{Supervised learning is a machine learning training
technique that optimizes a model based on examples input-output
pairs.}. Although initially promising, single layer perceptrons were
not able to train on multiple classes of patterns and were eventually
proven incapable of learning a XOR function \footnote{XOR (Exclusive
or, $x\oplus y$) is a logical operation that is true only if its
arguments differ.} in the book \textit{Perceptrons}
\cite{book:minsky1969perceptrons}, as the way they worked was by
``separating'' data linearly. This lead to a stagnation in machine
learning research dubbed ``AI winter'', until the proposal of
\textbf{backpropagation\footnote{Backpropagation is a method of fine
tuning a neural network based on the error rate obtained from previous
runs of the program. It will be discussed in detail later in this
thesis.}} by Paul John Werbos in 1975 \cite{book:werbos1975beyond}.

A renewed interest in the field lead to the development of the
Cresceptron \cite{article:Cresceptron} in 1992, a method of training
large networks with pooling layers (\textbf{max-pooling}) and down-sampling.
GPU\footnote{GPU - Graphics Processing Units is a specialized
electronic circuit, a central part of modern computers which excels in
    efficient computation of algorithms which process large blocks of data
    parallelly, thus exceling in machine learning applications.} usage made
    possible the training of larger networks, while new types of networks
    emerged such as the \textbf{Recurrent Neural Networks (RNNs)}.
    \textbf{Convolutional Neural Networks} have recently proven to be
    far superior for image classification tasks.

    In recent years, neural 


    \subsection{Basics}
    \subsubsection{Summary}

    One can think of ANNs as a directed graph, with a collection of nodes
    which are densely connected (called \textbf{artificial neurons}),
    transimiting signals to each other. These nodes are usually organized
    in sets of layers, with signals moving in one direction
    (i.e. \textit{feed forward networks}) through weighted connections.
    Signals received on a single neuron are real numbers, and the output
    of a single neuron is the output of an aggregation of a non-linear
    function of the sum of its inputs. This function is called an
    \textit{activation function} and its results are propagated to all of
    the nodes outgoing connections.  Thus, each neuron can be thought as a
    simple processing unit. The weighted connections between nodes might
    have an excitatory or inhibitory effect, based on these weights which
    can be positive, negative or very close to zero, having no effect
    \cite[Chap. ~ 1]{book:Gurney1997AnIT}. While training, example
    data is passed through the input layer and gets radically transformed
    through the layers until it reaches the output layer. The weights and
    other trainable parameters are then adjusted until the training data
    consistently reaches satisfactory results.

    \begin{figure}[h!]
    \centering
    % \input{tikz_figures/chap_nn/node_activation.tex}
    \scalebox{.8}{\input{tikz_figures/chap_nn/simple_net.tex}}
    \caption{A simple neural network demonstrating the layered structure and
    and flow of data from input to output.}
    \label{fig:simple_nn_demo}
    \end{figure}
    \newpage
    % \section{Backpropagation}
    \subsubsection{Building Blocks}
    % \begin{figure}[t!]
    %   \centering
    %   % \input{tikz_figures/chap_nn/node_activation.tex}
    %   \scalebox{.8}{\input{tikz_figures/chap_nn/simple_nn_weighted.tex}}
    %   \caption{A simple neural network demonstrating the layered structure and
    %   and flow of data from input to output.}
    %   \label{fig:nn_activation}
    % \end{figure}

    As mentioned before, the building blocks of ANNs are its artificial
    neurons organized into layers. In  \fref{fig:simple_nn_demo} a basic
    ANN is presented, demonstrating the different layers that are typically
    present in a feedforward neural network \footnote{Feedforward Neural Networks
    (FNNs) are the simplest type of neural networks, with the information moving
    only in one direction (``forward'' through the layers), without any loops or
    cycles \cite{article:SCHMID}. Different types of neural networks are discussed
    later.} while the building blocks are widely considered \cite{article:SCHMID} to
    be:
    \begin{itemize}
    \item \textbf{Input layer} This layer's purpose is to act as an
    entrypoint to the neural network and performs no computations. Data moves from
    this layer to the hidden layer succeeding it.
    \item \textbf{Hidden layer} Networks typically have one or more
    hidden layers, in which some computation takes place. Data moves from the input
    layer to a hidden layer where it is transformed and together with its weights
    moves to the next hidden layer or the output layer.
    \item \textbf{Output layer} Transformed data ``exits'' the network here,
    where it can pass through some function to reach the desired output
    format
    \item \textbf{Edges and Weights} Each node in a layer is connected
    to a set (usually all) of the nodes of the following layer with a
    weighted edge. Data from nodes $1 ... n$ of layer $k$ will be the input
    of node $j$ of layer $l$, multiplied by a weight $W_{ij}$. 
    \item \textbf{Activation Functions} An activation function takes as
    input some form of aggregate (usually the weighted sum) of the signals
  arriving at a node and produces an output. This function is typically
  nonlinear and differentiable for reasons which will be discussed later.
\item \textbf{Learning} The learning process in a ANN involves modifying
  its weights and other learnable parameters to improve the accuracy
  of the result on the output layer. Learning usually involves a cost
  function which is evaluated on a predefined basis and adjustments are
  made accordingly. One of the most widespread learning techniques is
  \textit{backpropagation}, where the error is propagated backwards
  through the network.
  
\end{itemize}
Along with the data from previous layers aggregated at a neuron, a bias
is typically added which acts in the same way the intercept does in a
linear equation. It adjusts the output of the activation function along
with the weighted sum of the inputs to the neuron. It is also a trainable
constant value provided to each node of a layer. Biases are node level
parameters and do not depend on values provided by previous layers.


\section{Mathematics useful in ANNs}

In this section some mathematical constructs which are widely
used in machine learening and ANNs are briefly presented.

\subsection{Gradient Descent}\label{sec:gradient_descent}

Gradient based optimizations methods are of great importance to
NNs as they are most common method of training these models. They
are tasked with minimizing or maximizing some function $f(\bm{x})$ which
is oftenly called \textbf{objective function}. In NN training
scenarios, where the goal is to minimize it the same function is also
commonly called a \textbf{cost}, \textbf{loss} or \textbf{error}
function\cite{book:Goodfellow}.

Gradient descent is a method of minimizing a function $f(x)$ and
finding a local minimum, given that its differentiable. The
derivative $f'(x)$ is the slope of $f(x)$ at $x$, so it specifies
how a change in x would provide a step towards the local minimum. For instance,
for small values of $\epsilon$, $f(x - \epsilon \sgn (f'(x)))$\footnote{
  $\sgn$ is the \textit{signum function}, a piecewise function which returns
  the sign of its input or 0 if input is 0. (i.e. $\sgn(-1) = -1$ and $\sgn(12) = 1$).
} is less than $f(x)$ and traversing the slope to ever decreasing
values if possible through following the direction with the opposite
sign of the derivate. This method is called \textbf{gradient descent}
and it was first proposed by Cauchy \cite{article:Cauchy} in 1847.
\begin{figure}[h!]
  \centering
  \input{tikz_figures/chap_nn/gradient_descent_3d.tex}
  \caption{Gradient descent in three dimensional space}
  \label{fig:gradient_descent_2d}
\end{figure}

\begin{definition}
  Consider a multi-variable function $F(\bm{x})$ which is \textbf{defined}
  and \textbf{differentiable} in a neighborhood of a point $a$. The value of
  $F(\bm{x})$ will decrease the fastest when moving from $a$ towards the
  negative gradient direction of $F$, which is $\bm{-\nabla} F(\bm{a})$.
  Thus when:
  \begin{equation}
    \label{eq:grad_simple}
    \bm{a}_{n+1} = \bm{a}_n - \gamma \nabla F(\bm{a}_n)
  \end{equation}
  then $F(\bm{a}_n) \geq F(\bm{a}_{n+1})$ and therefore the values
  of $F(\bm{a})$ move towards the local minimum. A sequence
  $\bm{x_0}, \bm{x_1}, \bm{x_2}, ... \bm{x_m}$ that follows the rule
  set by \eref{eq:grad_simple} will lead to the monotic sequence
  $F(\bm{x}_0) \geq F(\bm{x}_1) \geq F(\bm{x}_2), ...$ and the
  sequence $\bm{x_n}$ will converge to the local minimum.
  If some special choices are made for $\gamma$\footnotemark{} the function is
  guaranteed to reach a local minimum. Additionaly, if the function
  is \textit{convex} local minima are global minima and gradient descent
  converges to a global solution.

  For a function to be convex, a line segment connecting two of its points
  must lay on or above its curve. Mathematically for two points $x_1$ and $x_2$,
  this can be expressed as
  \begin{equation}
    \label{eq:convex_func}
    f(\lambda x_1 + (1 - \lambda)x_2) \leq \lambda f(x_1) + (1-\lambda)f(x_2)
  \end{equation}
  where $\lambda$ is a location on a section line and $0 \leq \lambda \leq 1$.
\end{definition}

\footnotetext{$\gamma$ in machine learning is also known as the ``learning rate''
  and its one of the hypermateres of NNs. Special choices made here include a selection
  of $\gamma$ via line search which satisfies the Wolfe conditions or the Barzilai-Borwein
  method \cite{book:optimization}
}
\subsection{Hadamard Product}\label{sec:hadamard}
The \textbf{Hadamard product}, also known as the \textbf{element-wise
product} is a matrix operation in which two matrices of the same
dimensions are multiplied to produce a matrix of equal dimensions,
where its $i,j$ elements are the product of elements $i, j$ of the
original two matrices. For two matrices A and B of dimensions $m\times
n$ the Hamarand product can be written as:
\begin{equation}
  \label{eq:hamarand}
  (A \odot B)_{ij} = (A)_{ij}(B)_{ij}
\end{equation}


% \subsection{Stochastic Gradient Descent (SGD)}\label{sec:stoch_grad_desc}




\section{Feedforward Networks}

\textbf{Feedforward neural networks (FNNs)} are the exemplary of ANNs, and the first to be
conceived and created by Rosenblatt in 1958 with the creation of the
perceptron \cite{article:Rosenblatt1958ThePA}. Their goal is to approximate
some function $f^{*}$; i.e. a classifier uses the function
$y=f^{*}(\bm{x})$ to map the input $\bm{x}$ to some category $y$. The
goal of a feedforward network would then be to define a mapping
$\bm{y} = f(\bm{x};~\bm{\theta})$ and train in a way that the values
of the parameter vector $\bm{\theta}$ provide the best approximation of
the function $f^{*}$.

These networks are called feedforward as information flows from the input layer
$\bm{x}$ through intermediate computational layers used to define $\bm{f}$ and
finally to the output $\bm{y}$. There are no loops providing (called
\textbf{feedback connections}) information from the output back into
the input or other intermediate layers of the network.

Feedforward networks can be thought as a chain of functions, composing
the final structure of the network. As an example, consider a network
with three of these functions as $f(\bm{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\bm{x})))$.
In this case, the exponents denotes the layer, with $f^{(1)}$ being the first
layer, $f^{(2)}$ the second etc. The total number of these functions is called
the \textbf{depth} of the NN and the terminology ``deep learning'' arose from
this layered structure.

During training the goal is to modify the parameters of the
network in a way that $f(\bm{x})$ closely matches $f^*(\bm{x})$. The training
set is composed of pairs of $\bm{x}$ and labels $y \approx f^*(\bm{x})$ and
the output of the network is evaluated at different training points. Training
data defines the exact result expected from each input $\bm{x}$, a value as close
as possible to $y$. The rest of the layers can have arbitary behaviours as long
as they transform the data in a way defined by the training goal. The
learning algorithm can ``use'' them in any way that is useful to it, and
the training data has no immediate effect on their behaviour. They are
thus called \textbf{``hidden layers''} as they do not produce any
meaningful result, related to the function \cite[p.~160]{book:Goodfellow}. 


% TODO: Describe deep learning (multiple layers?)
\subsection{Perceptrons, learning algorithms and simple NNs}\label{sec:perceptron}

The perceptron serves as great precursory example to
neural networks, as it introduces many concepts that are
common in more complex NN paradigms. It is in fact a simple
neuron, a computational unit, which takes a binary vector
as input and produces a binary output.

\begin{figure}[h!]
  \centering
  \input{tikz_figures/chap_nn/simple_perceptron.tex}
  \label{fig:simple_perceptron}
  \caption{A simple perceptron}
\end{figure}


Inputs are multiplied with \textit{weights}, and the output
of the neuron is either 0 or 1, determined by whether the
weighted sum of its inputs $\sum_i w_i a_i$ is greater than
a threshold value $threshold$. All of these numbers are real numbers
and a parameter of the neuron itself. The algebraic form of
this can be written as:



\begin{equation*}
  \text{Output} = \begin{cases}
    0 &\text{if } \sum_i w_i a_i \leq \text{threshold} \\
    1 &\text{if } \sum_i w_i a_i > \text{threshold}
  \end{cases}
\end{equation*}

By modifying the values of the threshold, the output
can be changed. The above equation can be written in
a more compact form by replacing the sums with the dot
product of the weights and the input vector,
$\sum_i w_i a_i = \bm{w} \cdot \bm{a}$. Moving
the threshold to the left side of the equation
and replacing it by what is referred to as the \textit{bias}
yields:

\begin{equation*}
  \text{Output} = \begin{cases}
    0 & if \bm{w} \cdot \bm{a} + b \leq 0 \\
    1 & if \bm{w} \cdot \bm{a} + b > 0
  \end{cases}
\end{equation*}

where $b$ = - threshold. The bias can be seen as a measure
of the tendency of the neuron to fire. Larger bias numbers
makes the neuron easy to activate and output 1, while smaller
ones require larger inputs and positive weights. 

\begin{remark}
  As mentioned before, perceptrons are able to solve problems
  which are linearly seperatable with the slope represented
  by the $\bm{w} \cdot \bm{a}$ term and the bias acting as
  the intercept. 
  This excludes problems which are not, the most famous being
  the classic XOR gate, as there does not exist one single
  line capable of seperating the predictions.
\end{remark}

Multiple perceptrons can be combined in a network to compute any
logical function, including the XOR gate. These kind of NNs are called
\textit{multilayer perceptrons}, and they are the basis modern
artificial neural networks. As discussed before, these neurons (or
nodes) are organized in layers and can have a depth based on the
number of these layers. Typically, at least 3 layers are present.

\subsection*{A more practical activation function}

So far, the way perceptrons transform data through a function
has been described, but while they can produce
sensible results as any other computing device, weights and
biases had to be manually configured. It is possible to introduce
a learning algorithm which does the tuning of these parameters
automatically, in response to input-output (training data) pairs.

The perceptrons described used the Heaviside step function
\cite{book:Abramowitz}, often denoted with $\textit{H}$. This function
has a binary nature, with its output value being 0 or 1 based on a
threshold, which presents a problem when fine tuning a perceptron or a
NN based on them. Minor changes in a weight or bias value can
completely alter the output and trigger a perceptron to flip, possibly
changing the output of the whole network. A better choice for an
activation function is the \textit{sigmoid function} $\sigma$ also
called the \textit{logistic function} which replaces the perceptron
with the sigmoid neuron \cite{book:nielsen}.

The sigmoid function is defined as:
\begin{equation}
    \sigma (x) = \frac{1}{1 + e^{-x}} = \frac{1}{1 + exp(-\sum_jw_jx_j-b)}
    \label{eq:sigmoid}
\end{equation}
It is easy to see the differences between these two functions when
plotted:
\begin{figure}[h]
  \centering
      \scalebox{1}{\input{tikz_figures/chap_nn/sigmoid_plot.tex}}
      \label{fig:sigmoid function and its derivative}
      \caption{Plot of the sigmoid function, its first derivative and
        the Heaviside step function.}
\end{figure}

For very negative and very positive values of the inputs, the sigmoid approximates
the behaviour of the step function, while for inputs around a neighborhood of 0
it differs by offering a smooth transition between 0 and 1 as an output. It also
is a differentiable function.
This properties allows an approximation of the output when the weights and
biases change slightly as:
\begin{equation*}
  \Delta_{\text{output}} \approx \sum_j\frac{\partial{\text{output}}}{\partial w_j} \Delta w_j + \frac{\partial{\text{output}}}{\partial b}\Delta b
\end{equation*}
This provides an estimation of how small changes in the biases and weights
will affect the output of the sigmoid neuron, making choices for their values
easy to calculate. 

As stated before, all of the computations that take place on a neuron
can be represented in a compact matrix form, as shown in \fref{fig:nn_activation}.
\begin{figure}[!h]
  \centering
  % \input{tikz_figures/chap_nn/node_activation.tex}
  \scalebox{1}{\input{tikz_figures/chap_nn/nn_activation.tex}}
  \caption{Input to a single neuron in a feedforward network. Here
    $\sigma$ represents the activation function (a sigmoid function in
    this case) and the exponents represent layers. The activation of a
    layer can be conveniently represented in matrix form. Biases are added
    to the input of the node.}
  \label{fig:nn_activation}
\end{figure}

\newpage
\subsection*{Learning}

Before defining how training works for a neural network, it is
imperative to describe an algorithm of quantifying far from the
target output the network is with its current biases and weights.
This algorithm is the \textit{cost function} described in \sref{sec:gradient_descent}.

If for all inputs in a vector $\bm{x}$ the desired output is known and is
the output of some function $f^{*}(\bm{x}) ~\text{or}~ y(\bm{x})$ and the
cost function can be defined as:
% \newpage
\begin{equation}
  \label{eq:cost_func_simple}
  C(\bm{w}, \bm{b}) = \frac{1}{2n} \sum_x ||y(\bm{x}) - \bm{a}||^2
\end{equation}

In this equation, $\bm{w}~\text{and}~\bm{b}$ are the weights and biases
of the network respectively and $\bm{a}$ is a vector of the outputs when
$\bm{x}$ is the input.

This cost function is also known as \textit{quadratic} or \textit{mean
  squared error (MSE)} and it is one of the most commonly used loss
functions. 
The goal of training is to minimize this function, and to achieve this
gradient descent and variations of this method are used.

Applying gradient descent to \eref{eq:cost_func_simple} yields:
\begin{equation}
  \label{eq:cost_func_grad}
  \nabla C = \frac{1}{n} \sum_x\nabla C_x
\end{equation}
where $x$ are the training samples.

\subsubsection*{Stochastic Gradient Descent (SGD)}
One of the most common variations is \textit{Stochastic Gradient
Descent (SGD)}. As training sets get larger, they become
computationally expensive especially considering that in
\eref{eq:cost_func_simple}, the total cost function $C =
\frac{1}{n}\sum_x C_x$, averaging the costs of each training sample.
The gradient of this function requires the computation of each $\nabla
C_x$ for each sample and then averaging them as $\nabla C =
\frac{1}{n}\sum_x\nabla C_x$ which can be prohibitive timewise.

SGD offers a solution to this problem, by drastically simplifying
the above process; instead of finding the exact value of $\nabla C$
, it estimates its value by randomly picking one sample \cite{article:bottou}.
This stochastic approximation adds noise, but it has been proven to
almost ensure convergence under mild conditions \cite{book:robbins}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.8\textwidth]{Figures/chap_nn/gradient_stoch.png}
  \caption{Noisy convergence of SGD compared to non-stochastic. Image
source \cite{article:image_gradient}.}
  \label{fig:gradient_descent_noisy}
\end{figure}

A smoother convergence by choosing a small number $m$ of samples which
is called a \textbf{mini-batch} and is the method most commonly used as
it offers a compromise between speed and smoothness of convergence.
In this case, \eref{eq:cost_func_grad} becomes:
\begin{equation}
  \label{eq:cost_func_stoch}
  \nabla C \approx \frac{1}{m} \sum_{i=1}^m \nabla C_{X_j}
\end{equation}

\subsubsection*{Training rules}

We can then define a training rule for the network in terms of
weights and biases and using the gradient of the cost function.
Applying SGD on \eref{eq:cost_func_simple} provides
the values of weights $w_k$  and biases $b_l$ that minimizes it. An update rule
for them can now be defined in terms of the cost function:
\begin{align}
  w_k \rightarrow w^{'}_k &= w_k - \frac{\eta}{m}\sum_j \frac{\partial C_{x_j}}{\partial w_k} \nonumber \\
  b_l \rightarrow b^{'}_l &= b_l - \frac{\eta}{m}\sum_j \frac{\partial C_{x_j}}{\partial b_l}
  \label{eq:update_w_b}
\end{align}

where $\eta$ is a small positive value, the \textbf{learning rate}. 

Having described the above, the process of training the NN can be summarized as:
\begin{enumerate}
\item The SGD is calculated for a random set number of training samples, the mini-batch.
\item Weights and biases are updated based on \eref{eq:yolo}.
\item The previous two steps are repeated until the training samples are exhausted.
\item When all samples are exhausted, the training \textbf{epoch} is complete, and the
  process starts again from the first step.
\item Training is usually said to be complete, when the value of  the cost function
  is below a set threshold. The NN is now considered trained.
\end{enumerate}

\subsection{Backpropagation}

Backpropagation is a method of computing the gradient of the loss function
with respect to all the weights and biases of the network, based on single
training examples, in contrast to more naive methods which compute the gradient
on each weight or bias individually.

Even though these days the term \textit{backpropagation} is sometimes
used colloquially to refer to the whole process of the learning algorithm,
strictly it is just an algorithm to compute the gradient, while SGD can be
used for the training itself \cite{book:Goodfellow}.

Historically, this method has been rediscovered multiple times, as early
as 1960 \cite{article:SCHMID} with its first implementation as
software in 1970 \cite[p.~229]{book:Goodfellow}, albeit as a method
for automatic differentation with no mention to neural networks. In
1974, Werbos proposed its usage in training neural
networks\cite{book:WerbosBackprop}, but it was not until 1986 when
\citet*{article:backprop} published their seminal paper ``Learning
representations by back-propagating errors'' which experimentally
showed that backpropagation can be used to train deep neural networks
faster than any existing technique up to that point.


Ultimately the goal of backpropagation is to compute the partial
derivatives $\frac{\partial C_X}{\partial w^l_{jk}} ~\text{and}~ \frac{\partial
  C_X}{\partial b^l_{j}}$ --- the partial derivates of the cost function
 with respect to weights and biases in the network.


\subsubsection*{Matrix Notation}

Before introducing the equations of backpropagation, it is
important to present a matrix notation for quantities in a
network which are used widely and unambigiuous. Consider the
equations presented in \fref{fig:nn_activation}. For a network
with $L$ layers, the connection of the $k^{th}$ neuron in the
$(l-1)^{th}$ layer with $j^{th}$ node in $l^{th}$ layer can be
written as $w_{jk}^l$. In a similar fashion, the bias of the
$j^{th}$ node in the $l^{th}$ layer can be represented as $b^l_j$
and the \textit{activation}\footnote{The activation of a neuron
  simply refers to the output of the activation function, taking as
  input the weighted sum of the inputs and the bias $a' = \sigma(wa +
  b)$
} of the same node is $a_j^l$. With this representations declared
it is possible to define the matrix representation of a layer $l$ as:
\begin{equation}
  \label{eq:layer}
  \bm{a^l} = \sigma(\bm{w^l} \bm{a^{l-1}} + \bm{b^l})
\end{equation}

where $\sigma$ is the activation function. At this point the
intermediate quantity $\bm{z^l} = \bm{w^la^{l-1}} + \bm{b^l}
\rightarrow \bm{a^l} = \sigma(\bm{z^l})$ commonly called the
\textit{weighted input} is worth declaring.

\subsubsection*{Fundamentals of Backpropagation}

Backpropagation has two special requirements for the
cost function. The first is that it must be able to be
written as an average $C = \frac{1}{n} \sum_XC_X$ of the
cost functions of individual training samples $x$. This
ensures that after computing the partial derivatives
$\frac{\partial C_X}{\partial w} \text{and} \frac{\partial C_X}{partial b}$
over the training set, it is possible to calculate 
$\frac{\partial C}{\partial w} \text{and} \frac{\partial C}{\partial b}$
by averaging over the set.

The second requirement is that the cost function $C$ can be
written as a function of the outputs of the NN. For example,
the MSE cost function described in \eref{eq:cost_func_simple}
for a single training example can be written as
\begin{equation}
  \label{eq:backprop_mse}
  C = \frac{1}{2}||\bm{y} - \bm{a^L}|| = \frac{1}{2}\sum_j(y_j - a^L_j)^2
\end{equation}

which in fact can be regarded a function of the outputs $\bm{a^L}$ alone,
as the training examples $y(x)$ have fixed values. Note that $L$ denotes the
number of layers in the networks.

\subsubsection*{Equations of backpropagation}

Before declaring the fundamental equations of backpropagation,
an important intermediate quantity called the \textbf{error} $\delta_j^l$
must be declared. In a neuron $j$, during activation, a small quantity $\Delta z_j^l$
is added as input and the neuron outputs $\sigma (z^l_j + \Delta z_j^l)$.
This error is propagated through the network and finally
the overall cost changes by $\frac{\partial C}{\partial z_j^l}\Delta z_j^l$.
The error can the be defined as:
\begin{equation}
  \label{eq:error_nn}
  \delta_j^l \equiv \frac{\partial C}{\partial z_j^l}
\end{equation}


The fundamental equations which govern the backpropagation process
are described below.
\begin{enumerate}
\item
  The first equation describes the error in the
  \textbf{output layer}:
  \begin{equation}
    \delta^L_j = \frac{\partial C}{\partial a_j^L} \sigma'(z_j^L)
  \end{equation}
  or equivalently in matrix form:
  \begin{equation}
    \label{eq:error_output}
    \delta^L = \bm{\nabla_a C} \odot \sigma' (\bm{z^L})
  \end{equation}
\item
  An equation for the error in terms of the error in the
  \textbf{next layer}:
  \begin{equation}
    \label{eq:error_layer}
    \delta^l = ((\bm{w^{l+1}})^T\bm{\delta^{l+1}})\odot \sigma'(\bm{z^l})
  \end{equation}
  This equation is convenient as it propagates the error backwards
  in the network. Knowning the error in the $(l+1)^{th}$ layer and
  by transposing the weight matrix the error can then move backwards
  through the activation function in layer $l$. This process can be
  repeated all the way back.
\item
  An equation describing the rate of change of the cost with respect to \textbf{biases}
  in the network:
  \begin{equation}
    \label{eq:error_bias}
    \frac{\partial C}{\partial b_j^l} = \delta^l_j \xrightarrow{\ref{eq:error_layer},~~ \ref{eq:error_output}} \frac{\partial C}{\partial b} = \delta 
  \end{equation}
\item
  An equation describing the rate of change of the cost with respect
  to \textbf{weights} in the network:
  \begin{equation}
    \label{eq:error_cost}
    \frac{\partial C}{\partial w_{jk}^l} = a^{l-1}_k\delta^l_j
  \end{equation}
  
\end{enumerate}

Finally, \eref{eq:error_bias} and \eref{eq:error_cost} have been
derived and can now be used in SGD or other learning algorithms.

The importance of backpropagation as a method arises from the fact that it is
possible to compute all the partial derivatives of the cost function
with just one forward and one backward pass through the network. Before
its emergence, training a network required computing these derivatives
for each weight and bias individually, which was prohibitively time consuming
for larger networks.


\subsubsection*{Learning algorithm using backpropagation}

The backpropagation algorithm can be represented in pseudocode
as shown below.


\begin{algorithm}
\caption{Backpropagation Algorithm}
\label{alg:Backprop}
\DontPrintSemicolon
\KwData{$\bm{x}$ the activation of the input layer $\bm{a^1}$}
\KwResult{The gradient of $C$ in terms of $\frac{\partial C}{\partial w_{jk}^l} = a^{l-1}_k\delta_j^l ~\text{and} ~ \frac{\partial C}{\partial b_j^l} = \delta_j^l$}
\Begin{
  % layers = $1, 2, 3, ... L$\;
  $layers \gets [2, 3, ... L ]$\;
  \tcp{Feedforward pass}
  \For {l in layers}{
    $z^l \gets w^la^{l-1} + b^l$\;
    $a^l \gets \sigma(z^l)$\;
  }
  \tcp{Output: Error vector}
  $\bm{\delta^l} \gets \nabla_a \bm{C} \odot \bm{\sigma'(z^L)}$ \;
  \tcp{Backpropagate the error}
  $layers\_reverse \gets [L-1, L-2, ... 2]$\;
  \For {l in layers\_reverse}{
    $ \bm{\delta^l} \gets ((\bm{(w^{l+1})^T\delta^{l+1}}) \odot \bm{\sigma'(z^l)}$\;
  }
  
  \tcp{Output}
  $\frac{\partial C}{\partial w_{jk}^l} \gets a^{l-1}_k\delta_j^l$\;
  $\frac{\partial C}{\partial b_j^l} \gets \delta_j^l$\;
}
\end{algorithm}

Consequently, combining backpropagation and a learning algorithm
such as a SGD, it is possible to train a network. For instance,
in a SGD with $m$ mini-batches case, the training rules, as described
in \eref{eq:update_w_b}, can be represented with the following pseudo-code:


\begin{algorithm}
\caption{Training Rules using SGD with backpropagation}
\label{alg:Training_Backprop}
\DontPrintSemicolon
\KwData{Sets of training examples}
\KwResult{Updated weights and biases based on gradient descent of the cost function}
\Begin{
  % layers = $1, 2, 3, ... L$\;
  $a^{x, l} \gets \text{set the activation for each sample}$\;
  \tcp{Pass the activation to the backpropagation from \Aref{alg:Backprop}
    and get the error as output}
  $\delta^{x,l} = ((w^{l+1})^T\delta^{x,l+1})\odot \sigma'(z^{x,l})$\;
  \tcp{Gradient Descent and updates for weights and biases}
  $layers\_reverse \gets [L-1, L-2, ... 2]$\;
  \For {l in layers\_reverse}{
  $ w^{l'} \gets w^l - \frac{\eta}{m}\sum_x\delta^{x,l}(a^{x,l-1})^T$\;
  $b^{l'} \gets b^l - \frac{\eta}{m}\sum_x\delta^{x,l}$\;
  }
}
\end{algorithm}

\clearpage
\subsection{Common ANN Architectures}

So far, the models discussed were simple perceptrons and multi-layer
perceptrons (MLPs), networks which are fully connected --- each neuron
in a layer is connected to all the neurons of the next layer (i.e. see
\fref{fig:simple_nn_demo}). This architecture does not take into
account that data used as input might carry some spatial structure, as
in the case of image recognition and computer vision.  In this
section, convolutional neural networks will be introduced briefly,
which take into account the spatially local input patterns of some
datasets, while at the same time reducing the number of free parameters
allowing for deeper networks. While the origin of these types of
networks can be traced to 1970, they were first established as a concept
by LeCun in the paper ``Gradient-based learning applied to document
recognition'' from \citet{article:lecun1998}.

\subsubsection{Convolutional Neural Networks (CNNs)}

As mentioned before, CNNs take advantage of the spatial structure that
some types of data have. For instance an image has a grid-topology,
which is ignored in the case of simple or deep MLPs and as a consequence it
inhibits learning by treating pixels which are spatially far, in the same
manner. CNNs architecture solve this problem by taking into account
the spatial characteristics of their input data. The basic concepts of
CNNs are \textit{local receptive fields, shared weights} and \textit{pooling}.
In this context, the input layer is considered to be 2-dimensional structure
typically a square.

\textbf{Local receptive fields}

Each neuron of the hidden layer is connected to a
small region of the input layer, typically a small square, which
is called the \textit{local receptive field} of the hidden neuron.
Each of these connections has a trainable weight, and the neuron has a bias.
The region is slid across the input layer, passing values to the hidden layer.

\textbf{Shared weights and biases}

All of the hidden neurons share the same weights and biases; with the activation
of the i, $k$th neuron begin:
\begin{equation}
  \label{eq:cnn_activation}
  \sigma \bigg(b + \sum_{l=0}^{r_x}\sum_{m=0}^{r_y}w_{l,m}a_{j+l,k+m}\bigg)
\end{equation}

where $\sigma$ represents the neural activation function, $a_{x,y}$
are the activations of the input at those coordinates and $r_x, r_y$
are the dimensions of the receptive field.

Sharing weights and biases allows for features to be detected
from input data, in conjuction with their spatial coordinates.
For this reason, the mapping of the input layer to the hidden
layer is called a \textit{feature map}, and the shared weights
and biases are often called a \textit{kernel} or \textit{filter}.
Typically, CNNs have several of these feature maps, one for each
feature they are configured (or learn) to detect.

\begin{figure}[H]
  \centering
  \input{tikz_figures/chap_nn/feature_maps.tex}
  \caption{Illustration of a convolutional layer with multiple feature maps.}
  \label{fig:feature_maps}
\end{figure}

\textbf{Pooling Layers} Pooling layers are typically
used immediately after the convolutional (hidden) layers.  It reduces
the dimensions of clusters of neuros into a signle neuron in the next
layer. The two most common pooling procedures are max-pooling which
outputs the maximum activation from the pooling region and the average
pooling which outputs the average value of the activations.

\begin{figure}[H]
  \centering
  \input{tikz_figures/chap_nn/pooling.tex}
  \caption{Illustration of the pooling layer following the feature maps}
  \label{fig:pooling}
\end{figure}

CNNs are importang for understanding Graph neural networks as they
introduce the concept of spatially aware data transformations, taking
into account the topology of the input data. This, albeit with some
changes, will be a critical concept when studying GNNs in the next chapter.

\subsubsection{Recurrent Neural Networks (RNNs)}

So far, only networks with a static quality were discussed ---
information flowed from the input layer towards the output. Recurrent
neural networks are types of networks were information can flow on
dynamically formed temporal connections, exhibiting changing
behaviour. As an example, in these types of networks, a neuron's
activation might not only depend on activations of previous hidden
layers, but it can also pocess an internal memory which affects, in part,
its activation based on previous activations. The defining characteristic
of RNNs is their change over time dynamically, while they can
pocess loops feeding activations back to the neurons after some
possible transformations, stored states or incorporate temporal delays.
They are most useful in applications where data or processes analyzed
change over item, such as in speech or natural language recognition. 


