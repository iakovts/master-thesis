\chapter{Transformers and self-attention}\label{sec:transformers}

\section{Attention and self-attention}

In neural networks, \textit{attention}\cite{article:attention} is a mechanism
that takes into account several of the inputs of the network
and attributes importance to them by means of different weights
and biases. Its function mimicks that of cognitive attention of
biological brains.

The main objective of attention in neural networks is to learn
to recognize which parts of the input are relevant and which are not.
For instance one of its first uses was in speech recognition and
natural language understanding \cite{bahdanau2014neural}, where
large data sequences are encountered.

Another technique described in the paper by \citet{article:attention}
is ``self-attention'' where is a mechanism which relates different positions
of the same input sequence to create a representation of this sequence.
