\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}{dummy.2}\protected@file@percent }
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{ii}{dummy.3}\protected@file@percent }
\@writefile{toc}{\vspace  {1em}}
\citation{article:image_gradient}
\citation{article:stefi}
\citation{site:embeddings}
\citation{article:attention}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{v}{dummy.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vi}{dummy.7}\protected@file@percent }
\citation{book:2008}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}{Introduction}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Graphs}{1}{section.10}\protected@file@percent }
\newlabel{sec:graphs}{{1.1}{1}{Graphs}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Introduction}{1}{subsection.11}\protected@file@percent }
\newlabel{u_simple_graph}{{1.1}{1}{}{theorem.12}{}}
\citation{book:2008}
\citation{article:bollobas}
\citation{book:Gary}
\newlabel{graph_def}{{1.2}{2}{}{theorem.14}{}}
\newlabel{eq:phi}{{1.1}{2}{}{equation.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An undirected pseudograph with labeled nodes and edges.\relax }}{3}{figure.caption.16}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SimplePseudograph}{{1.1}{3}{An undirected pseudograph with labeled nodes and edges.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Adjacency Matrix}{4}{subsection.19}\protected@file@percent }
\newlabel{eq:adj_mat}{{1.2}{4}{}{equation.21}{}}
\citation{book:algebraic}
\citation{book:Newman}
\newlabel{fig:simple_adj_demo}{{1.2a}{5}{Multigraph with no loops and multiple edges.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:simple_adj_demo}{{a}{5}{Multigraph with no loops and multiple edges.\relax }{figure.caption.22}{}}
\newlabel{fig:compl_adj_demo}{{1.2b}{5}{Mutligraph with loops and multiple edges.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:compl_adj_demo}{{b}{5}{Mutligraph with loops and multiple edges.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Two undirected multigraphs.\relax }}{5}{figure.caption.22}\protected@file@percent }
\newlabel{fig:two multigraphs}{{1.2}{5}{Two undirected multigraphs.\relax }{figure.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Adjacency matrix for Figure\nobreakspace  {}\ref  {fig:simple_adj_demo}\relax }}{5}{table.caption.23}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Adjacency matrix for Figure\nobreakspace  {}\ref  {fig:compl_adj_demo}\relax }}{6}{table.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2.1}Adjacency List}{6}{subsubsection.26}\protected@file@percent }
\newlabel{table:adj_list_ex}{{\caption@xref {table:adj_list_ex}{ on input line 245}}{6}{Adjacency List}{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Adjacency list for Figure\nobreakspace  {}\ref  {fig:compl_adj_demo}\relax }}{6}{table.caption.27}\protected@file@percent }
\citation{article:ChebNet}
\citation{book:Newman}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Graph Laplacian}{7}{subsection.29}\protected@file@percent }
\newlabel{sec:laplacian}{{1.1.3}{7}{Graph Laplacian}{subsection.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Edge weights}{7}{subsection.30}\protected@file@percent }
\newlabel{sec:edge_weights}{{1.1.4}{7}{Edge weights}{subsection.30}{}}
\citation{book:Newman}
\newlabel{table:weighted_adj}{{1.3b}{8}{Corresponding adjacency matrix.\relax }{figure.caption.31}{}}
\newlabel{sub@table:weighted_adj}{{b}{8}{Corresponding adjacency matrix.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Simple example of an unordered graph with weighted edges\relax }}{8}{figure.caption.31}\protected@file@percent }
\newlabel{fig:weighted_graph}{{1.3}{8}{Simple example of an unordered graph with weighted edges\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Distance between nodes and shortest paths}{8}{subsection.32}\protected@file@percent }
\newlabel{table:weighted_adj}{{\caption@xref {table:weighted_adj}{ on input line 385}}{9}{}{figure.caption.35}{}}
\newlabel{sub@table:weighted_adj}{{}{9}{}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces A graph with a maximum path of 3 (nodes 1 to 6).\relax }}{9}{figure.caption.35}\protected@file@percent }
\newlabel{fig:weighted_graph}{{1.4}{9}{A graph with a maximum path of 3 (nodes 1 to 6).\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Node and edge properties}{9}{subsection.36}\protected@file@percent }
\newlabel{table:properties_example}{{\caption@xref {table:properties_example}{ on input line 463}}{11}{}{figure.caption.38}{}}
\newlabel{sub@table:properties_example}{{}{11}{}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Example graph of a small classroom with labeled edges and nodes\relax }}{11}{figure.caption.38}\protected@file@percent }
\newlabel{fig:properties_example}{{1.5}{11}{Example graph of a small classroom with labeled edges and nodes\relax }{figure.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces A) Node properties B) Edge properties\relax }}{11}{table.caption.39}\protected@file@percent }
\newlabel{tbl:Node and edge properties}{{1.4}{11}{A) Node properties B) Edge properties\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.5}{\ignorespaces Graph Properties\relax }}{11}{table.caption.40}\protected@file@percent }
\citation{book:Jost2007}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Network Dynamics}{12}{section.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Simple Contagion Dynamics (SIS)}{12}{subsection.42}\protected@file@percent }
\newlabel{sec:SIS}{{1.2.1}{12}{Simple Contagion Dynamics (SIS)}{subsection.42}{}}
\newlabel{eq:prob_inf}{{1.3}{12}{Simple Contagion Dynamics (SIS)}{equation.43}{}}
\newlabel{eq:prob_rec}{{1.4}{12}{Simple Contagion Dynamics (SIS)}{equation.44}{}}
\citation{murphy}
\citation{albert2002statistical}
\citation{erdHos1960evolution}
\newlabel{eq:sis}{{1.5}{13}{Simple Contagion Dynamics (SIS)}{equation.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Complex Contagion Dynamics --- Planck SIS}{13}{subsection.46}\protected@file@percent }
\newlabel{sec:planck}{{1.2.2}{13}{Complex Contagion Dynamics --- Planck SIS}{subsection.46}{}}
\newlabel{eq:plank}{{1.6}{13}{Complex Contagion Dynamics --- Planck SIS}{equation.47}{}}
\newlabel{eq:zeta}{{1.7}{13}{Complex Contagion Dynamics --- Planck SIS}{equation.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Models of network formation}{13}{section.49}\protected@file@percent }
\newlabel{sec:creating_nw}{{1.3}{13}{Models of network formation}{section.49}{}}
\citation{article:gilbert}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}The Barab{\'a}si-Albert Model}{14}{subsection.50}\protected@file@percent }
\newlabel{eq:ba_nw}{{1.8}{14}{The Barab{\'a}si-Albert Model}{equation.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Erd{\H {o}}s-R{\'e}nyi Model}{14}{subsection.52}\protected@file@percent }
\newlabel{eq:er}{{1.9}{14}{Erd{\H {o}}s-R{\'e}nyi Model}{equation.53}{}}
\citation{article:McCulloch1943}
\citation{article:Rosenblatt1958ThePA}
\citation{book:minsky1969perceptrons}
\citation{book:werbos1975beyond}
\citation{article:Cresceptron}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Neural Networks}{15}{chapter.54}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{literature}{{2}{15}{Neural Networks}{chapter.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Historical Background}{15}{section.55}\protected@file@percent }
\citation{article:cire}
\citation{graves2008offline}
\citation{book:Gurney1997AnIT}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Basics}{16}{subsection.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}Summary}{16}{subsubsection.61}\protected@file@percent }
\citation{article:SCHMID}
\citation{article:SCHMID}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A simple neural network demonstrating the layered structure and and flow of data from input to output.\relax }}{17}{figure.caption.62}\protected@file@percent }
\newlabel{fig:simple_nn_demo}{{2.1}{17}{A simple neural network demonstrating the layered structure and and flow of data from input to output.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.2}Building Blocks}{17}{subsubsection.63}\protected@file@percent }
\citation{book:Goodfellow}
\citation{article:Cauchy}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Mathematics useful in ANNs}{18}{section.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Gradient Descent}{18}{subsection.66}\protected@file@percent }
\newlabel{sec:gradient_descent}{{2.2.1}{18}{Gradient Descent}{subsection.66}{}}
\citation{book:optimization}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Gradient descent in three dimensional space\relax }}{19}{figure.caption.68}\protected@file@percent }
\newlabel{fig:gradient_descent_2d}{{2.2}{19}{Gradient descent in three dimensional space\relax }{figure.caption.68}{}}
\newlabel{eq:grad_simple}{{2.1}{19}{}{equation.70}{}}
\newlabel{eq:convex_func}{{2.2}{19}{}{equation.72}{}}
\citation{article:Rosenblatt1958ThePA}
\citation{book:Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Hadamard Product}{20}{subsection.73}\protected@file@percent }
\newlabel{sec:hadamard}{{2.2.2}{20}{Hadamard Product}{subsection.73}{}}
\newlabel{eq:hamarand}{{2.3}{20}{Hadamard Product}{equation.74}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Feedforward Networks}{20}{section.75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Perceptrons, learning algorithms and simple NNs}{21}{subsection.76}\protected@file@percent }
\newlabel{sec:perceptron}{{2.3.1}{21}{Perceptrons, learning algorithms and simple NNs}{subsection.76}{}}
\newlabel{fig:simple_perceptron}{{\caption@xref {fig:simple_perceptron}{ on input line 287}}{21}{Perceptrons, learning algorithms and simple NNs}{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A simple perceptron\relax }}{21}{figure.caption.77}\protected@file@percent }
\citation{book:Abramowitz}
\citation{book:nielsen}
\newlabel{eq:sigmoid}{{2.4}{23}{A more practical activation function}{equation.80}{}}
\newlabel{fig:sigmoid function and its derivative}{{\caption@xref {fig:sigmoid function and its derivative}{ on input line 376}}{23}{A more practical activation function}{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Plot of the sigmoid function, its first derivative and the Heaviside step function.\relax }}{23}{figure.caption.81}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Input to a single neuron in a feedforward network. Here $\sigma $ represents the activation function (a sigmoid function in this case) and the exponents represent layers. The activation of a layer can be conveniently represented in matrix form. Biases are added to the input of the node.\relax }}{24}{figure.caption.82}\protected@file@percent }
\newlabel{fig:nn_activation}{{2.5}{24}{Input to a single neuron in a feedforward network. Here $\sigma $ represents the activation function (a sigmoid function in this case) and the exponents represent layers. The activation of a layer can be conveniently represented in matrix form. Biases are added to the input of the node.\relax }{figure.caption.82}{}}
\newlabel{eq:cost_func_simple}{{2.5}{24}{Learning}{equation.84}{}}
\citation{article:bottou}
\citation{book:robbins}
\citation{article:image_gradient}
\citation{article:image_gradient}
\newlabel{eq:cost_func_grad}{{2.6}{25}{Learning}{equation.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Noisy convergence of SGD compared to non-stochastic. Image source \cite  {article:image_gradient}.\relax }}{25}{figure.caption.87}\protected@file@percent }
\newlabel{fig:gradient_descent_noisy}{{2.6}{25}{Noisy convergence of SGD compared to non-stochastic. Image source \cite {article:image_gradient}.\relax }{figure.caption.87}{}}
\citation{book:Goodfellow}
\newlabel{eq:cost_func_stoch}{{2.7}{26}{Stochastic Gradient Descent (SGD)}{equation.88}{}}
\newlabel{eq:update_w_b}{{2.8}{26}{Training rules}{equation.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Backpropagation}{26}{subsection.96}\protected@file@percent }
\citation{article:SCHMID}
\citation{book:Goodfellow}
\citation{book:WerbosBackprop}
\citation{article:backprop}
\newlabel{eq:layer}{{2.9}{27}{Matrix Notation}{equation.99}{}}
\newlabel{eq:backprop_mse}{{2.10}{28}{Fundamentals of Backpropagation}{equation.101}{}}
\newlabel{eq:error_nn}{{2.11}{28}{Equations of backpropagation}{equation.103}{}}
\newlabel{eq:error_output}{{2.13}{28}{Equations of backpropagation}{equation.106}{}}
\newlabel{eq:error_layer}{{2.14}{28}{Equations of backpropagation}{equation.108}{}}
\newlabel{eq:error_bias}{{2.15}{29}{Equations of backpropagation}{equation.110}{}}
\newlabel{eq:error_cost}{{2.16}{29}{Equations of backpropagation}{equation.112}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Backpropagation Algorithm\relax }}{30}{algocf.116}\protected@file@percent }
\newlabel{alg:Backprop}{{1}{30}{Learning algorithm using backpropagation}{algocf.116}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Training Rules using SGD with backpropagation\relax }}{30}{algocf.119}\protected@file@percent }
\newlabel{alg:Training_Backprop}{{2}{30}{Learning algorithm using backpropagation}{algocf.119}{}}
\citation{article:lecun1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Common ANN Architectures}{31}{subsection.120}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.1}Convolutional Neural Networks (CNNs)}{31}{subsubsection.121}\protected@file@percent }
\newlabel{eq:cnn_activation}{{2.17}{31}{Convolutional Neural Networks (CNNs)}{equation.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Illustration of a convolutional layer with multiple feature maps.\relax }}{32}{figure.caption.123}\protected@file@percent }
\newlabel{fig:feature_maps}{{2.7}{32}{Illustration of a convolutional layer with multiple feature maps.\relax }{figure.caption.123}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Illustration of the pooling layer following the feature maps\relax }}{32}{figure.caption.124}\protected@file@percent }
\newlabel{fig:pooling}{{2.8}{32}{Illustration of the pooling layer following the feature maps\relax }{figure.caption.124}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.2}Recurrent Neural Networks (RNNs)}{33}{subsubsection.125}\protected@file@percent }
\citation{article:TGNNM}
\citation{article:zhou}
\citation{article:sperduti}
\citation{article:geomDeep}
\citation{hamilton2017representation,article:zhang}
\citation{mikolov2013efficient,mikolov2013distributed}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Graph Neural Networks (GNNs)}{34}{chapter.126}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{GNN}{{3}{34}{Graph Neural Networks (GNNs)}{chapter.126}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{34}{section.127}\protected@file@percent }
\citation{article:xiong,article:bognini}
\citation{article:wieder}
\citation{article:sanchez}
\citation{article:weng}
\citation{article:zhiwei}
\citation{article:chaudhary}
\citation{article:monti}
\citation{article:jiang}
\citation{xu2018powerful}
\citation{article:duvenaud}
\citation{article:stefi}
\citation{article:stefi}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Dopamine molecule represented as a graph. Different types of nodes represent different atoms and different edges represent the chemical bonds present in the molecule. Image source \citet  {article:stefi}.\relax }}{36}{figure.caption.128}\protected@file@percent }
\newlabel{fig:dopamine}{{3.1}{36}{Dopamine molecule represented as a graph. Different types of nodes represent different atoms and different edges represent the chemical bonds present in the molecule. Image source \citet {article:stefi}.\relax }{figure.caption.128}{}}
\citation{site:embeddings}
\citation{site:embeddings}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}From CNNs to Graph Neural Networks}{37}{section.130}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Challenges}{37}{subsection.131}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Creating embeddings}{37}{subsection.132}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Embedding of nodes $u$ and $v$ to low dimensional vector space. Image source \cite  {site:embeddings}\relax }}{38}{figure.caption.134}\protected@file@percent }
\newlabel{fig:node_embedding}{{3.2}{38}{Embedding of nodes $u$ and $v$ to low dimensional vector space. Image source \cite {site:embeddings}\relax }{figure.caption.134}{}}
\newlabel{fig:feature_map}{{\caption@xref {fig:feature_map}{ on input line 176}}{38}{Creating embeddings}{figure.caption.138}{}}
\newlabel{sub@fig:feature_map}{{}{38}{Creating embeddings}{figure.caption.138}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A graph and its corresponding feature vector. The feature vector contains the features from all the nodes, in this case simply a real number.\relax }}{38}{figure.caption.138}\protected@file@percent }
\newlabel{fig:combo_feature}{{3.3}{38}{A graph and its corresponding feature vector. The feature vector contains the features from all the nodes, in this case simply a real number.\relax }{figure.caption.138}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Initial Implementations}{38}{subsection.139}\protected@file@percent }
\citation{defferrard2016convolutional}
\citation{article:hammond}
\newlabel{eq:polynomial_basic}{{3.1}{39}{Initial Implementations}{equation.140}{}}
\newlabel{eq:convo_pol}{{3.2}{39}{Initial Implementations}{equation.141}{}}
\newlabel{eq:conv_ex}{{3.3}{39}{Initial Implementations}{equation.142}{}}
\newlabel{eq:poly_deg}{{3.4}{39}{Initial Implementations}{equation.143}{}}
\newlabel{eq:poly_fin}{{3.5}{39}{Initial Implementations}{equation.144}{}}
\citation{defferrard2016convolutional}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.1}Algorithmic Computation using polynomials}{40}{subsubsection.147}\protected@file@percent }
\citation{article:messageP}
\citation{velickovic2017graph}
\citation{article:attention}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Embedding Computation for a GNN using $K$ polynomial filter layers.\relax }}{41}{algocf.150}\protected@file@percent }
\newlabel{alg:embedding_comp}{{3}{41}{Algorithmic Computation using polynomials}{algocf.150}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Modern Graph Neural Networks}{41}{section.151}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Embeddings}{41}{subsection.152}\protected@file@percent }
\citation{velickovic2017graph}
\citation{velickovic2017graph}
\citation{velickovic2017graph}
\citation{velickovic2017graph}
\citation{daigavane2021understanding}
\newlabel{fig:multiheadGAN}{{\caption@xref {fig:multiheadGAN}{ on input line 365}}{42}{Embeddings}{figure.caption.154}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Multi-head attention mechanism for representation extraction}}{42}{figure.caption.154}\protected@file@percent }
\newlabel{eq:gat_compact}{{3.6}{42}{Embeddings}{equation.155}{}}
\citation{zhang2018end,ying2018hierarchical,lee2019self}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Learning}{43}{subsection.156}\protected@file@percent }
\newlabel{eq:categoricalCE}{{3.7}{43}{Learning}{equation.157}{}}
\citation{murphy}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments and Results}{44}{chapter.158}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{experiments}{{4}{44}{Experiments and Results}{chapter.158}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Background}{44}{section.159}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Fundamental ideas of this approach}{44}{section.160}\protected@file@percent }
\newlabel{eq:y_indepe}{{4.2}{45}{Fundamental ideas of this approach}{equation.162}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Description of goals}{45}{section.163}\protected@file@percent }
\newlabel{eq:M_mimicks}{{4.3}{45}{Description of goals}{equation.164}{}}
\citation{murphy}
\citation{murphy}
\newlabel{eq:outcomes_y}{{4.4}{46}{Description of goals}{equation.165}{}}
\newlabel{eq:loss_func}{{4.5}{46}{Description of goals}{equation.166}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Architecture of the GNN}{46}{section.167}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Structure}{46}{subsection.168}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The GAT GNN Architecture}}{46}{figure.caption.169}\protected@file@percent }
\newlabel{fig:GATGNN}{{4.1}{46}{The GAT GNN Architecture}{figure.caption.169}{}}
\citation{velickovic2017graph}
\newlabel{eq:input_layer}{{4.6}{47}{Structure}{equation.170}{}}
\newlabel{eq:first_att}{{4.7}{47}{Structure}{equation.171}{}}
\newlabel{eq:final_out}{{4.8}{47}{Structure}{equation.172}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Attention Mechanism}{47}{subsection.173}\protected@file@percent }
\newlabel{eq:att_coeff}{{4.9}{47}{Attention Mechanism}{equation.174}{}}
\newlabel{eq:att_aggr}{{4.10}{47}{Attention Mechanism}{equation.175}{}}
\citation{murphy}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Loss Function}{48}{subsection.176}\protected@file@percent }
\newlabel{eq:cross_loss}{{4.11}{48}{Loss Function}{equation.177}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Benchmarking and performance}{48}{subsection.178}\protected@file@percent }
\newlabel{eq:pearson}{{4.12}{48}{Benchmarking and performance}{equation.179}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experiments}{48}{section.180}\protected@file@percent }
\newlabel{sec:exps}{{4.5}{48}{Experiments}{section.180}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Generating Data}{49}{subsection.181}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}SIS}{49}{subsection.182}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Results for BA network running SIS dynamics}}{50}{figure.caption.183}\protected@file@percent }
\newlabel{fig:ba_sis}{{4.2}{50}{Results for BA network running SIS dynamics}{figure.caption.183}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Planck SIS}{50}{subsection.184}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Results for BA network running planck SIS dynamics}}{51}{figure.caption.185}\protected@file@percent }
\newlabel{fig:ba_plancksis}{{4.3}{51}{Results for BA network running planck SIS dynamics}{figure.caption.185}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion of results}{52}{section.186}\protected@file@percent }
\newlabel{sec:discussion}{{4.6}{52}{Discussion of results}{section.186}{}}
\@writefile{toc}{\vspace  {2em}}
\citation{article:attention}
\citation{bahdanau2014neural}
\citation{article:attention}
\citation{article:attention}
\citation{article:attention}
\citation{article:attention}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Transformers and self-attention}{53}{appendix.187}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:transformers}{{A}{53}{Transformers and self-attention}{appendix.187}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Attention and self-attention}{53}{section.188}\protected@file@percent }
\newlabel{Attention}{{A.1}{53}{Attention and self-attention}{section.188}{}}
\citation{velickovic2017graph}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Multi-head attention module as described above. Image source \cite  {article:attention}\relax }}{54}{figure.caption.189}\protected@file@percent }
\newlabel{fig:multihead}{{A.1}{54}{Multi-head attention module as described above. Image source \cite {article:attention}\relax }{figure.caption.189}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Attention in Graph Attention Networks}{54}{section.190}\protected@file@percent }
\newlabel{sec:attentionGAT}{{A.2}{54}{Attention in Graph Attention Networks}{section.190}{}}
\newlabel{eq:att_coeff}{{A.1}{54}{Attention in Graph Attention Networks}{equation.191}{}}
\newlabel{eq:full_gan}{{A.2}{55}{Attention in Graph Attention Networks}{equation.192}{}}
\newlabel{eq:final_feature}{{A.3}{55}{Attention in Graph Attention Networks}{equation.193}{}}
\newlabel{eq:final_mult}{{A.4}{55}{Attention in Graph Attention Networks}{equation.194}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional Results}{56}{appendix.195}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:res_add}{{B}{56}{Additional Results}{appendix.195}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}SIS on a $G(n, p)$ network}{56}{section.196}\protected@file@percent }
\newlabel{sec:res_gnp}{{B.1}{56}{SIS on a $G(n, p)$ network}{section.196}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Results for $G(n,p)$ network running SIS dynamics}}{57}{figure.caption.197}\protected@file@percent }
\newlabel{fig:gnp_sis}{{B.1}{57}{Results for $G(n,p)$ network running SIS dynamics}{figure.caption.197}{}}
\@writefile{toc}{\vspace  {2em}}
\bibstyle{unsrtnat}
\bibdata{Bibliography}
\bibcite{article:image_gradient}{{1}{2018}{{Carpenter et~al.}}{{Carpenter, Cohen, Jarrell, and Huang}}}
\bibcite{article:stefi}{{2}{2019}{{Stefi et~al.}}{{Stefi, Barth, David, Quessette, Weisser, and Watel}}}
\bibcite{site:embeddings}{{3}{}{{sit}}{{}}}
\bibcite{article:attention}{{4}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{book:2008}{{5}{2008}{{Adrian~Bondy}}{{}}}
\bibcite{article:bollobas}{{6}{2002}{{Bollobas and Szemerédi}}{{}}}
\bibcite{book:Gary}{{7}{2012}{{Gary~Chartrand}}{{}}}
\bibcite{book:algebraic}{{8}{}{{Ashikhmin and Barg}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{58}{dummy.198}\protected@file@percent }
\newlabel{Bibliography}{{5}{58}{SIS on a $G(n, p)$ network}{dummy.198}{}}
\bibcite{book:Newman}{{9}{2018}{{Newman}}{{}}}
\bibcite{article:ChebNet}{{10}{2016}{{Defferrard et~al.}}{{Defferrard, Bresson, and Vandergheynst}}}
\bibcite{book:Jost2007}{{11}{2007}{{Jost}}{{}}}
\bibcite{murphy}{{12}{2021}{{Murphy et~al.}}{{Murphy, Laurence, and Allard}}}
\bibcite{albert2002statistical}{{13}{2002}{{Albert and Barab{\'a}si}}{{}}}
\bibcite{erdHos1960evolution}{{14}{1960}{{Erd{\H {o}}s et~al.}}{{Erd{\H {o}}s, R{\'e}nyi, et~al.}}}
\bibcite{article:gilbert}{{15}{1959}{{Gilbert}}{{}}}
\bibcite{article:McCulloch1943}{{16}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{article:Rosenblatt1958ThePA}{{17}{1958}{{Rosenblatt}}{{}}}
\bibcite{book:minsky1969perceptrons}{{18}{1969}{{Minsky and Papert}}{{}}}
\bibcite{book:werbos1975beyond}{{19}{1975}{{Werbos}}{{}}}
\bibcite{article:Cresceptron}{{20}{1992}{{Weng et~al.}}{{Weng, Ahuja, and Huang}}}
\bibcite{article:cire}{{21}{2010}{{Cireşan et~al.}}{{Cireşan, Meier, Gambardella, and Schmidhuber}}}
\bibcite{graves2008offline}{{22}{2008}{{Graves and Schmidhuber}}{{}}}
\bibcite{book:Gurney1997AnIT}{{23}{1997}{{Gurney}}{{}}}
\bibcite{article:SCHMID}{{24}{2015}{{Schmidhuber}}{{}}}
\bibcite{book:Goodfellow}{{25}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, and Courville}}}
\bibcite{article:Cauchy}{{26}{1847}{{Cauchy}}{{}}}
\bibcite{book:optimization}{{27}{2005}{{Liqun~Qi}}{{}}}
\bibcite{book:Abramowitz}{{28}{1972}{{Abramowitz and Stegun}}{{}}}
\bibcite{book:nielsen}{{29}{2015}{{Nielsen}}{{}}}
\bibcite{article:bottou}{{30}{2010}{{Bottou}}{{}}}
\bibcite{book:robbins}{{31}{1971}{{Robbins and Siegmund}}{{}}}
\bibcite{book:WerbosBackprop}{{32}{1994}{{Werbos}}{{}}}
\bibcite{article:backprop}{{33}{1986}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{article:lecun1998}{{34}{1998}{{Lecun et~al.}}{{Lecun, Bottou, Bengio, and Haffner}}}
\bibcite{article:TGNNM}{{35}{2009}{{Scarselli et~al.}}{{Scarselli, Gori, Tsoi, Hagenbuchner, and Monfardini}}}
\bibcite{article:zhou}{{36}{2020}{{Zhou et~al.}}{{Zhou, Cui, Hu, Zhang, Yang, Liu, Wang, Li, and Sun}}}
\bibcite{article:sperduti}{{37}{1997}{{Sperduti and Starita}}{{}}}
\bibcite{article:geomDeep}{{38}{2017}{{Bronstein et~al.}}{{Bronstein, Bruna, LeCun, Szlam, and Vandergheynst}}}
\bibcite{hamilton2017representation}{{39}{2017}{{Hamilton et~al.}}{{Hamilton, Ying, and Leskovec}}}
\bibcite{article:zhang}{{40}{2017}{{Zhang et~al.}}{{Zhang, Yin, Zhu, and Zhang}}}
\bibcite{mikolov2013efficient}{{41}{2013{}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013distributed}{{42}{2013{}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{article:xiong}{{43}{2021}{{Xiong et~al.}}{{Xiong, Xiong, Chen, Jiang, and Zheng}}}
\bibcite{article:bognini}{{44}{2021}{{Bongini et~al.}}{{Bongini, Bianchini, and Scarselli}}}
\bibcite{article:wieder}{{45}{2020}{{Wieder et~al.}}{{Wieder, Kohlbacher, Kuenemann, Garon, Ducrot, Seidel, and Langer}}}
\bibcite{article:sanchez}{{46}{2020}{{Sanchez-Gonzalez et~al.}}{{Sanchez-Gonzalez, Godwin, Pfaff, Ying, Leskovec, and Battaglia}}}
\bibcite{article:weng}{{47}{2021}{{{Weng Lo} et~al.}}{{{Weng Lo}, {Layeghy}, {Sarhan}, {Gallagher}, and {Portmann}}}}
\bibcite{article:zhiwei}{{48}{2021}{{Guo et~al.}}{{Guo, Tang, Guo, Yu, Alazab, and Shalaginov}}}
\bibcite{article:chaudhary}{{49}{2019}{{Chaudhary et~al.}}{{Chaudhary, Mittal, and Arora}}}
\bibcite{article:monti}{{50}{2019}{{Monti et~al.}}{{Monti, Frasca, Eynard, Mannion, and Bronstein}}}
\bibcite{article:jiang}{{51}{2021}{{Jiang and Luo}}{{}}}
\bibcite{xu2018powerful}{{52}{2018}{{Xu et~al.}}{{Xu, Hu, Leskovec, and Jegelka}}}
\bibcite{article:duvenaud}{{53}{2015}{{Duvenaud et~al.}}{{Duvenaud, Maclaurin, Iparraguirre, Bombarell, Hirzel, Aspuru-Guzik, and Adams}}}
\bibcite{defferrard2016convolutional}{{54}{2016}{{Defferrard et~al.}}{{Defferrard, Bresson, and Vandergheynst}}}
\bibcite{article:hammond}{{55}{2011}{{Hammond et~al.}}{{Hammond, Vandergheynst, and Gribonval}}}
\bibcite{article:messageP}{{56}{2017}{{Gilmer et~al.}}{{Gilmer, Schoenholz, Riley, Vinyals, and Dahl}}}
\bibcite{velickovic2017graph}{{57}{2017}{{Velickovic et~al.}}{{Velickovic, Cucurull, Casanova, Romero, Lio, and Bengio}}}
\bibcite{daigavane2021understanding}{{58}{2021}{{Daigavane et~al.}}{{Daigavane, Ravindran, and Aggarwal}}}
\bibcite{zhang2018end}{{59}{2018}{{Zhang et~al.}}{{Zhang, Cui, Neumann, and Chen}}}
\bibcite{ying2018hierarchical}{{60}{2018}{{Ying et~al.}}{{Ying, You, Morris, Ren, Hamilton, and Leskovec}}}
\bibcite{lee2019self}{{61}{2019}{{Lee et~al.}}{{Lee, Lee, and Kang}}}
\bibcite{bahdanau2014neural}{{62}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
